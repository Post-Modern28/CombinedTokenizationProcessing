{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11519343,"sourceType":"datasetVersion","datasetId":7224502},{"sourceId":11530954,"sourceType":"datasetVersion","datasetId":7232418}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin(token = \"hf_\") \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T15:54:55.193063Z","iopub.execute_input":"2025-04-24T15:54:55.193854Z","iopub.status.idle":"2025-04-24T15:54:55.240220Z","shell.execute_reply.started":"2025-04-24T15:54:55.193818Z","shell.execute_reply":"2025-04-24T15:54:55.239667Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"/kaggle/input/model-with-trained-embeddings/saved_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T15:54:55.241620Z","iopub.execute_input":"2025-04-24T15:54:55.241857Z","iopub.status.idle":"2025-04-24T15:55:10.253885Z","shell.execute_reply.started":"2025-04-24T15:54:55.241840Z","shell.execute_reply":"2025-04-24T15:55:10.253343Z"}},"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at /kaggle/input/model-with-trained-embeddings/saved_model were not used when initializing LlamaForCausalLM: ['model.extra_embedding_1.weight', 'model.extra_embedding_2.weight']\n- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from transformers import LlamaForCausalLM\nimport torch.nn as nn\n\nclass LlamaWithExtraEmbeddings(LlamaForCausalLM):\n    def __init__(self, config):\n        super().__init__(config)\n        \n        original_vocab_size, embedding_dim = self.model.embed_tokens.weight.shape\n        self.model.extra_embedding_1 = nn.Embedding(original_vocab_size, embedding_dim)\n        self.model.extra_embedding_2 = nn.Embedding(original_vocab_size, embedding_dim)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T15:55:10.254523Z","iopub.execute_input":"2025-04-24T15:55:10.255085Z","iopub.status.idle":"2025-04-24T15:55:10.260545Z","shell.execute_reply.started":"2025-04-24T15:55:10.255063Z","shell.execute_reply":"2025-04-24T15:55:10.259849Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"model = LlamaWithExtraEmbeddings.from_pretrained(\"/kaggle/input/model-with-trained-embeddings/saved_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T15:55:10.262071Z","iopub.execute_input":"2025-04-24T15:55:10.262553Z","iopub.status.idle":"2025-04-24T15:55:10.374563Z","shell.execute_reply.started":"2025-04-24T15:55:10.262526Z","shell.execute_reply":"2025-04-24T15:55:10.374073Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T15:55:10.375288Z","iopub.execute_input":"2025-04-24T15:55:10.375508Z","iopub.status.idle":"2025-04-24T15:55:10.381475Z","shell.execute_reply.started":"2025-04-24T15:55:10.375492Z","shell.execute_reply":"2025-04-24T15:55:10.380914Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"LlamaWithExtraEmbeddings(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 768)\n    (layers): ModuleList(\n      (0-11): 12 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((768,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((768,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((768,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n    (extra_embedding_1): Embedding(32000, 768)\n    (extra_embedding_2): Embedding(32000, 768)\n  )\n  (lm_head): Linear(in_features=768, out_features=32000, bias=False)\n)"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"## Mean of embeddings","metadata":{}},{"cell_type":"code","source":"from typing import List, Optional, Union\nfrom cachetools import Cache\nimport types\nimport torch\n\ndef modified_forward(\n    self,\n    input_ids: torch.LongTensor = None,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n    inputs_embeds: Optional[torch.FloatTensor] = None,\n    labels: Optional[torch.LongTensor] = None,\n    use_cache: Optional[bool] = None,\n    output_attentions: Optional[bool] = None,\n    output_hidden_states: Optional[bool] = None,\n    return_dict: Optional[bool] = None,\n    cache_position: Optional[torch.LongTensor] = None,\n    num_logits_to_keep: int = 0,\n    **kwargs\n):\n    if input_ids is None and inputs_embeds is None:\n        raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n    batch_size = input_ids.shape[0]\n    combined_embeds = []\n\n    for batch_idx in range(batch_size):\n        str_input_ids = \" \".join([str(i) for i in input_ids[batch_idx].tolist()])\n        input_ids_parts = str_input_ids.split(\" 200000 \")\n\n        embed_list = []\n        count = 0\n\n        if self.embedding_config.get(\"use_bpe\", False):\n            bpe_ids = torch.tensor([list(map(int, input_ids_parts[0].split(\" \")))], device=input_ids.device)\n            bpe_emb = self.model.embed_tokens(bpe_ids)\n            embed_list.append(bpe_emb)\n            count += 1\n\n        if self.embedding_config.get(\"use_wordpiece\", False):\n            wp_ids = torch.tensor([list(map(int, input_ids_parts[1].split(\" \")))], device=input_ids.device)\n            wp_emb = self.model.extra_embedding_1(wp_ids)\n            embed_list.append(wp_emb)\n            count += 1\n\n        if self.embedding_config.get(\"use_unigram\", False):\n            uni_ids = torch.tensor([list(map(int, input_ids_parts[2].split(\" \")))], device=input_ids.device)\n            uni_emb = self.model.extra_embedding_2(uni_ids)\n            embed_list.append(uni_emb)\n            count += 1\n\n        min_len = min(e.shape[1] for e in embed_list)\n        embed_list = [e[:, :min_len, :] for e in embed_list]\n        averaged = sum(embed_list) / count\n        combined_embeds.append(averaged)\n\n    inputs_embeds = torch.cat(combined_embeds, dim=0)\n\n    if attention_mask is not None:\n        attention_mask = attention_mask[:, :inputs_embeds.shape[1]]\n\n    return self.original_forward(\n        input_ids=None,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        past_key_values=past_key_values,\n        inputs_embeds=inputs_embeds,\n        use_cache=use_cache,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n        cache_position=cache_position,\n        labels=labels,\n        num_logits_to_keep=num_logits_to_keep,\n        **kwargs\n    )\n\n    \nmodel.original_forward = model.forward\nmodel.forward = types.MethodType(modified_forward, model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T15:55:10.382199Z","iopub.execute_input":"2025-04-24T15:55:10.382369Z","iopub.status.idle":"2025-04-24T15:55:10.402192Z","shell.execute_reply.started":"2025-04-24T15:55:10.382355Z","shell.execute_reply":"2025-04-24T15:55:10.401650Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"model.embedding_config = {\n    \"use_bpe\": True,         # original\n    \"use_wordpiece\": True,   # wordpiece_tokenizer\n    \"use_unigram\": True      # unigram_tokenizer\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T15:55:10.402779Z","iopub.execute_input":"2025-04-24T15:55:10.403010Z","iopub.status.idle":"2025-04-24T15:55:10.427550Z","shell.execute_reply.started":"2025-04-24T15:55:10.402986Z","shell.execute_reply":"2025-04-24T15:55:10.426915Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from datasets import load_dataset\n\nds = load_dataset(\"mlabonne/FineTome-Alpaca-100k\", split=\"train\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T15:55:10.428298Z","iopub.execute_input":"2025-04-24T15:55:10.428498Z","iopub.status.idle":"2025-04-24T15:55:14.885765Z","shell.execute_reply.started":"2025-04-24T15:55:10.428477Z","shell.execute_reply":"2025-04-24T15:55:14.884939Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/408 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce7fbbaff19b45dc83306c3d2112b4e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/89.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1aeb86b65bcd4f6ab5f61b28377209ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/100000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49a69189359544daa2fcca27db5e29e6"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"def tokenize(examples, tokenizer):\n    texts = [f\"### Instruction: {instruction}\\n### Response: {output}\" \n             for instruction, output in zip(examples['instruction'], examples['output'])]\n    \n    tokenized = tokenizer(\n        texts,\n        truncation=True,\n        max_length=1024,\n        padding=\"max_length\",\n        return_tensors=None\n    )\n        \n    # Add labels for causal language modeling\n    tokenized[\"labels\"] = [ids.copy() for ids in tokenized[\"input_ids\"]]\n    return tokenized","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T15:55:14.886646Z","iopub.execute_input":"2025-04-24T15:55:14.887213Z","iopub.status.idle":"2025-04-24T15:55:14.892117Z","shell.execute_reply.started":"2025-04-24T15:55:14.887193Z","shell.execute_reply":"2025-04-24T15:55:14.891424Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from transformers import PreTrainedTokenizerFast\nfrom transformers import AutoTokenizer\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM\n\noriginal_tokenizer = AutoTokenizer.from_pretrained(\"nickypro/tinyllama-110M\")\nwordpiece_tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/nlp-tokenizers/tokenizers/wordpiece_tokenizer\")\nunigram_tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/nlp-tokenizers/tokenizers/unigram_tokenizer\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T15:55:14.894924Z","iopub.execute_input":"2025-04-24T15:55:14.895221Z","iopub.status.idle":"2025-04-24T15:55:15.998291Z","shell.execute_reply.started":"2025-04-24T15:55:14.895202Z","shell.execute_reply":"2025-04-24T15:55:15.997482Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/686 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a44066521804002a2ac586317925c6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12d8d53f27fb4bb6bf870d428b7bbf4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8aeb5e8d507242f08ff9cd827fad57eb"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import datasets\nimport gc\n\n\noriginal_tokenizer.pad_token = original_tokenizer.eos_token\nwordpiece_tokenizer.pad_token = wordpiece_tokenizer.eos_token\nunigram_tokenizer.pad_token = unigram_tokenizer.eos_token\n\norig_tokenized_text = ds.map(lambda examples: tokenize(examples, original_tokenizer), batched=True, remove_columns=['instruction',\"source\",\"score\",'output'])\norig_tokenized_text = orig_tokenized_text.rename_column(\"input_ids\", \"input_ids1\")\norig_tokenized_text = orig_tokenized_text.rename_column(\"attention_mask\", \"attention_mask1\")\norig_tokenized_text = orig_tokenized_text.rename_column(\"labels\", \"labels1\")\n\nwordpiece_tokenized_text = ds.map(lambda examples: tokenize(examples, wordpiece_tokenizer), batched=True, remove_columns=['instruction',\"source\",\"score\",'output'])\nwordpiece_tokenized_text = wordpiece_tokenized_text.rename_column(\"input_ids\", \"input_ids2\")\nwordpiece_tokenized_text = wordpiece_tokenized_text.rename_column(\"attention_mask\", \"attention_mask2\")\nwordpiece_tokenized_text = wordpiece_tokenized_text.rename_column(\"labels\", \"labels2\")\n\nunigram_tokenized_text = ds.map(lambda examples: tokenize(examples, unigram_tokenizer), batched=True, remove_columns=['instruction',\"source\",\"score\",'output'])\nunigram_tokenized_text = unigram_tokenized_text.rename_column(\"input_ids\", \"input_ids3\")\nunigram_tokenized_text = unigram_tokenized_text.rename_column(\"attention_mask\", \"attention_mask3\")\nunigram_tokenized_text = unigram_tokenized_text.rename_column(\"labels\", \"labels3\")\n\n# Unify the tokenized datasets\n# Remove the 'token_type_ids' column from each dataset to avoid duplication\n# orig_tokenized_text = orig_tokenized_text.remove_columns(['token_type_ids'])\nwordpiece_tokenized_text = wordpiece_tokenized_text.remove_columns(['token_type_ids'])\nunigram_tokenized_text = unigram_tokenized_text.remove_columns(['token_type_ids'])\n\n# Unify the tokenized datasets\nunified_tokenized_text = datasets.concatenate_datasets([orig_tokenized_text, wordpiece_tokenized_text, unigram_tokenized_text], axis=1)\n\n\ndel orig_tokenized_text\ndel wordpiece_tokenized_text\ndel unigram_tokenized_text\n# del unigram_tokenized_text\n\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T15:55:15.999072Z","iopub.execute_input":"2025-04-24T15:55:15.999338Z","iopub.status.idle":"2025-04-24T15:59:41.513158Z","shell.execute_reply.started":"2025-04-24T15:55:15.999315Z","shell.execute_reply":"2025-04-24T15:59:41.512517Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e33165f9090d4304b301f977a2838b5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"109235c38f78415dae6b5bebf7039e6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"327f14ca304640508c85eee15e81376d"}},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"32"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"def concat_rows(row, max_length=1024):\n    sep = [200000]\n\n    combined_input_ids = row['input_ids1'] + sep + row['input_ids2'] + sep + row['input_ids3']\n    combined_attention_mask = row['attention_mask1'] + sep + row['attention_mask2'] + sep + row['attention_mask3']\n    combined_labels = row['labels1'] \n\n    return {\n        'input_ids': combined_input_ids,\n        'attention_mask': combined_attention_mask,\n        'labels': combined_labels\n    }\n\n\nunified_tokenized_text = unified_tokenized_text.map(\n    lambda row: concat_rows(row, max_length=1024),\n    remove_columns=['input_ids1', 'input_ids2', 'input_ids3', 'attention_mask1', 'attention_mask2', 'attention_mask3', 'labels1', 'labels2', 'labels3']\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T15:59:41.513905Z","iopub.execute_input":"2025-04-24T15:59:41.514208Z","iopub.status.idle":"2025-04-24T16:06:30.566557Z","shell.execute_reply.started":"2025-04-24T15:59:41.514189Z","shell.execute_reply":"2025-04-24T16:06:30.565674Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38bc759369c149a6996c188ccfbb095e"}},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"\nfrom torch.utils.data import DataLoader\nfrom transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n\n\ndef multi_tokenizer_data_collator(features):\n    \"\"\"\n    Custom data collator for handling input from multiple tokenizers\n    with separator tokens (200000) preserved.\n    \"\"\"\n    # Input IDs with batch dimension\n    input_ids = torch.stack([torch.tensor(f[\"input_ids\"]) for f in features])\n    attention_mask = torch.stack([torch.tensor(f[\"attention_mask\"]) for f in features])\n    labels = torch.stack([torch.tensor(f[\"labels\"]) for f in features])\n\n    # print(input_ids.shape, labels.shape, attention_mask.shape)\n    \n    return {\n        \"input_ids\": input_ids,  \n        \"attention_mask\": attention_mask,\n        \"labels\": labels\n    }\n\n# Create the data collator\ndata_collator = multi_tokenizer_data_collator\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T16:06:30.567853Z","iopub.execute_input":"2025-04-24T16:06:30.568138Z","iopub.status.idle":"2025-04-24T16:06:32.947279Z","shell.execute_reply.started":"2025-04-24T16:06:30.568119Z","shell.execute_reply":"2025-04-24T16:06:32.946731Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nloader = DataLoader(\n    unified_tokenized_text,\n    batch_size=1,\n    collate_fn=multi_tokenizer_data_collator\n)\n\nbatch = next(iter(loader))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T16:06:32.947940Z","iopub.execute_input":"2025-04-24T16:06:32.948117Z","iopub.status.idle":"2025-04-24T16:06:33.002371Z","shell.execute_reply.started":"2025-04-24T16:06:32.948102Z","shell.execute_reply":"2025-04-24T16:06:33.001834Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nbatch = {k: v.to(device) for k, v in batch.items()}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T16:06:33.003127Z","iopub.execute_input":"2025-04-24T16:06:33.003341Z","iopub.status.idle":"2025-04-24T16:06:39.901777Z","shell.execute_reply.started":"2025-04-24T16:06:33.003324Z","shell.execute_reply":"2025-04-24T16:06:39.901193Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"model.embedding_config = {\n    \"use_bpe\": True,         # original\n    \"use_wordpiece\": False,   # wordpiece_tokenizer\n    \"use_unigram\": True      # unigram_tokenizer\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T16:06:39.902445Z","iopub.execute_input":"2025-04-24T16:06:39.902647Z","iopub.status.idle":"2025-04-24T16:06:39.906506Z","shell.execute_reply.started":"2025-04-24T16:06:39.902632Z","shell.execute_reply":"2025-04-24T16:06:39.905903Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    output = model(\n        input_ids=batch[\"input_ids\"],\n        attention_mask=batch[\"attention_mask\"],\n        labels=batch[\"labels\"]\n    )\n\nprint(\"Loss:\", output.loss.item())\nprint(\"Logits shape:\", output.logits.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T16:06:39.907274Z","iopub.execute_input":"2025-04-24T16:06:39.907439Z","iopub.status.idle":"2025-04-24T16:06:41.446628Z","shell.execute_reply.started":"2025-04-24T16:06:39.907426Z","shell.execute_reply":"2025-04-24T16:06:41.445997Z"}},"outputs":[{"name":"stderr","text":"`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"name":"stdout","text":"Loss: 7.5166850090026855\nLogits shape: torch.Size([1, 1024, 32000])\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## Concatenation of embeddings","metadata":{}},{"cell_type":"code","source":"from transformers import LlamaForCausalLM\nimport torch.nn as nn\n\nclass LlamaWithExtraEmbeddings(LlamaForCausalLM):\n    def __init__(self, config):\n        super().__init__(config)\n        self.embedding_config = {\n            \"use_bpe\": True,         # original\n            \"use_wordpiece\": True,   # wordpiece_tokenizer\n            \"use_unigram\": True      # unigram_tokenizer\n        }\n\n        original_vocab_size, embedding_dim = self.model.embed_tokens.weight.shape\n        self.model.extra_embedding_1 = nn.Embedding(original_vocab_size, embedding_dim)\n        self.model.extra_embedding_2 = nn.Embedding(original_vocab_size, embedding_dim)\n        active_embeddings = sum([\n            self.embedding_config.get(\"use_bpe\", False),\n            self.embedding_config.get(\"use_wordpiece\", False),\n            self.embedding_config.get(\"use_unigram\", False)\n        ])\n        embedding_dim = self.model.embed_tokens.embedding_dim\n        self.embedding_projection = nn.Linear(embedding_dim * active_embeddings, embedding_dim)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T16:06:41.447374Z","iopub.execute_input":"2025-04-24T16:06:41.447635Z","iopub.status.idle":"2025-04-24T16:06:41.453606Z","shell.execute_reply.started":"2025-04-24T16:06:41.447619Z","shell.execute_reply":"2025-04-24T16:06:41.452777Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"model = LlamaWithExtraEmbeddings.from_pretrained(\"/kaggle/input/model-with-trained-embeddings/saved_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T16:06:41.454548Z","iopub.execute_input":"2025-04-24T16:06:41.454795Z","iopub.status.idle":"2025-04-24T16:06:41.563243Z","shell.execute_reply.started":"2025-04-24T16:06:41.454768Z","shell.execute_reply":"2025-04-24T16:06:41.562709Z"}},"outputs":[{"name":"stderr","text":"Some weights of LlamaWithExtraEmbeddings were not initialized from the model checkpoint at /kaggle/input/model-with-trained-embeddings/saved_model and are newly initialized: ['embedding_projection.bias', 'embedding_projection.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T16:06:41.563844Z","iopub.execute_input":"2025-04-24T16:06:41.564093Z","iopub.status.idle":"2025-04-24T16:06:41.569461Z","shell.execute_reply.started":"2025-04-24T16:06:41.564076Z","shell.execute_reply":"2025-04-24T16:06:41.568718Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"LlamaWithExtraEmbeddings(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 768)\n    (layers): ModuleList(\n      (0-11): 12 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((768,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((768,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((768,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n    (extra_embedding_1): Embedding(32000, 768)\n    (extra_embedding_2): Embedding(32000, 768)\n  )\n  (lm_head): Linear(in_features=768, out_features=32000, bias=False)\n  (embedding_projection): Linear(in_features=2304, out_features=768, bias=True)\n)"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"def concat_forward(self, input_ids, attention_mask=None, **kwargs):\n    batch_size = input_ids.shape[0]\n    combined_embeds = []\n\n    for batch_idx in range(batch_size):\n        str_input_ids = \" \".join([str(i) for i in input_ids[batch_idx].tolist()])\n        input_ids_parts = str_input_ids.split(\" 200000 \")\n\n        embed_list = []\n\n        if self.embedding_config.get(\"use_bpe\", False):\n            bpe_ids = torch.tensor([list(map(int, input_ids_parts[0].split(\" \")))], device=input_ids.device)\n            bpe_emb = self.model.embed_tokens(bpe_ids)\n            embed_list.append(bpe_emb)\n\n        if self.embedding_config.get(\"use_wordpiece\", False):\n            wp_ids = torch.tensor([list(map(int, input_ids_parts[1].split(\" \")))], device=input_ids.device)\n            wp_emb = self.model.extra_embedding_1(wp_ids)\n            embed_list.append(wp_emb)\n\n        if self.embedding_config.get(\"use_unigram\", False):\n            uni_ids = torch.tensor([list(map(int, input_ids_parts[2].split(\" \")))], device=input_ids.device)\n            uni_emb = self.model.extra_embedding_2(uni_ids)\n            embed_list.append(uni_emb)\n\n        min_len = min(e.shape[1] for e in embed_list)\n        embed_list = [e[:, :min_len, :] for e in embed_list]\n\n        concat_emb = torch.cat(embed_list, dim=-1)\n\n        # Project back\n        projected_embeds = self.embedding_projection(concat_emb)\n        combined_embeds.append(projected_embeds)\n\n    inputs_embeds = torch.cat(combined_embeds, dim=0)\n\n    if attention_mask is not None:\n        attention_mask = attention_mask[:, :inputs_embeds.shape[1]]\n\n    return self.original_forward(\n        input_ids=None,\n        attention_mask=attention_mask,\n        inputs_embeds=inputs_embeds,\n        **kwargs\n    )\n\n\nmodel.original_forward = model.forward\nmodel.forward = types.MethodType(concat_forward, model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T16:06:41.570263Z","iopub.execute_input":"2025-04-24T16:06:41.570449Z","iopub.status.idle":"2025-04-24T16:06:41.587993Z","shell.execute_reply.started":"2025-04-24T16:06:41.570435Z","shell.execute_reply":"2025-04-24T16:06:41.587447Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nloader = DataLoader(\n    unified_tokenized_text,\n    batch_size=1,\n    collate_fn=multi_tokenizer_data_collator\n)\n\nbatch = next(iter(loader))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T16:06:41.588658Z","iopub.execute_input":"2025-04-24T16:06:41.588952Z","iopub.status.idle":"2025-04-24T16:06:41.626153Z","shell.execute_reply.started":"2025-04-24T16:06:41.588925Z","shell.execute_reply":"2025-04-24T16:06:41.625667Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    output = model(\n        input_ids=batch[\"input_ids\"],\n        attention_mask=batch[\"attention_mask\"],\n        labels=batch[\"labels\"]\n    )\n\nprint(\"Loss:\", output.loss.item())\nprint(\"Logits shape:\", output.logits.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T16:06:41.626851Z","iopub.execute_input":"2025-04-24T16:06:41.627099Z","iopub.status.idle":"2025-04-24T16:06:43.513279Z","shell.execute_reply.started":"2025-04-24T16:06:41.627084Z","shell.execute_reply":"2025-04-24T16:06:43.512605Z"}},"outputs":[{"name":"stdout","text":"Loss: 12.096101760864258\nLogits shape: torch.Size([1, 1024, 32000])\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"## Weighted average","metadata":{}},{"cell_type":"code","source":"from transformers import LlamaForCausalLM\nimport torch.nn as nn\n\nclass LlamaWithExtraEmbeddings(LlamaForCausalLM):\n    def __init__(self, config):\n        super().__init__(config)\n        \n        original_vocab_size, embedding_dim = self.model.embed_tokens.weight.shape\n        self.model.extra_embedding_1 = nn.Embedding(original_vocab_size, embedding_dim)\n        self.model.extra_embedding_2 = nn.Embedding(original_vocab_size, embedding_dim)\n        self.embedding_config = {\n            \"use_bpe\": True,\n            \"use_wordpiece\": True,\n            \"use_unigram\": True,\n            \"weights\": {\n                \"bpe\": 0.5,\n                \"wordpiece\": 0.,\n                \"unigram\": 0.5\n            }\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T17:20:31.440222Z","iopub.execute_input":"2025-04-24T17:20:31.440824Z","iopub.status.idle":"2025-04-24T17:20:31.445364Z","shell.execute_reply.started":"2025-04-24T17:20:31.440802Z","shell.execute_reply":"2025-04-24T17:20:31.444728Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"model = LlamaWithExtraEmbeddings.from_pretrained(\"/kaggle/input/model-with-trained-embeddings/saved_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T17:20:33.427317Z","iopub.execute_input":"2025-04-24T17:20:33.427572Z","iopub.status.idle":"2025-04-24T17:20:33.515398Z","shell.execute_reply.started":"2025-04-24T17:20:33.427554Z","shell.execute_reply":"2025-04-24T17:20:33.514846Z"}},"outputs":[],"execution_count":76},{"cell_type":"code","source":"def weighted_avg_forward(self, input_ids, attention_mask=None, **kwargs):\n    batch_size = input_ids.shape[0]\n    combined_embeds = []\n\n    weights = self.embedding_config.get(\"weights\", {})\n    total_weight = sum(weights.get(key, 0.0) for key in [\"bpe\", \"wordpiece\", \"unigram\"])\n\n    for batch_idx in range(batch_size):\n        str_input_ids = \" \".join([str(i) for i in input_ids[batch_idx].tolist()])\n        input_ids_parts = str_input_ids.split(\" 200000 \")\n\n        embed_list = []\n        weight_list = []\n\n        if self.embedding_config.get(\"use_bpe\", False):\n            bpe_ids = torch.tensor([list(map(int, input_ids_parts[0].split(\" \")))], device=input_ids.device)\n            bpe_emb = self.model.embed_tokens(bpe_ids)\n            embed_list.append(bpe_emb)\n            weight_list.append(weights.get(\"bpe\", 0.0))\n\n        if self.embedding_config.get(\"use_wordpiece\", False):\n            wp_ids = torch.tensor([list(map(int, input_ids_parts[1].split(\" \")))], device=input_ids.device)\n            wp_emb = self.model.extra_embedding_1(wp_ids)\n            embed_list.append(wp_emb)\n            weight_list.append(weights.get(\"wordpiece\", 0.0))\n\n        if self.embedding_config.get(\"use_unigram\", False):\n            uni_ids = torch.tensor([list(map(int, input_ids_parts[2].split(\" \")))], device=input_ids.device)\n            uni_emb = self.model.extra_embedding_2(uni_ids)\n            embed_list.append(uni_emb)\n            weight_list.append(weights.get(\"unigram\", 0.0))\n\n\n        min_len = min(e.shape[1] for e in embed_list)\n        embed_list = [e[:, :min_len, :] for e in embed_list]\n\n\n        norm_weights = [w / total_weight for w in weight_list]\n        # norm_weights = weight_list\n\n        weighted_emb = sum(w * emb for w, emb in zip(norm_weights, embed_list))\n        combined_embeds.append(weighted_emb)\n\n\n    inputs_embeds = torch.cat(combined_embeds, dim=0)\n\n    if attention_mask is not None:\n        attention_mask = attention_mask[:, :inputs_embeds.shape[1]]\n\n    return self.original_forward(\n        input_ids=None,\n        attention_mask=attention_mask,\n        inputs_embeds=inputs_embeds,\n        **kwargs\n    )\n\n\nmodel.original_forward = model.forward\nmodel.forward = types.MethodType(weighted_avg_forward, model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T17:20:35.223730Z","iopub.execute_input":"2025-04-24T17:20:35.224437Z","iopub.status.idle":"2025-04-24T17:20:35.233439Z","shell.execute_reply.started":"2025-04-24T17:20:35.224412Z","shell.execute_reply":"2025-04-24T17:20:35.232800Z"}},"outputs":[],"execution_count":77},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nloader = DataLoader(\n    unified_tokenized_text,\n    batch_size=1,\n    collate_fn=multi_tokenizer_data_collator\n)\n\nbatch = next(iter(loader))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T17:20:38.034175Z","iopub.execute_input":"2025-04-24T17:20:38.034930Z","iopub.status.idle":"2025-04-24T17:20:38.044082Z","shell.execute_reply.started":"2025-04-24T17:20:38.034902Z","shell.execute_reply":"2025-04-24T17:20:38.043360Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    output = model(\n        input_ids=batch[\"input_ids\"],\n        attention_mask=batch[\"attention_mask\"],\n        labels=batch[\"labels\"]\n    )\n\nprint(\"Loss:\", output.loss.item())\nprint(\"Logits shape:\", output.logits.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T17:20:39.669900Z","iopub.execute_input":"2025-04-24T17:20:39.670407Z","iopub.status.idle":"2025-04-24T17:20:41.192949Z","shell.execute_reply.started":"2025-04-24T17:20:39.670384Z","shell.execute_reply":"2025-04-24T17:20:41.192130Z"}},"outputs":[{"name":"stdout","text":"Loss: 7.512155532836914\nLogits shape: torch.Size([1, 1024, 32000])\n","output_type":"stream"}],"execution_count":79},{"cell_type":"markdown","source":"| BPE Weight | Wordpiece Weight | Unigram Weight | Loss                    |\n|------------|------------------|----------------|-------------------------|\n| 0.9        | 0.1              | 0.1            | 7.031398773193359       |\n| 0.5        | 0.4              | 0.1            | 7.331758499145508       |\n| 1.0        | 0.0              | 0.0            | 7.0994086265563965      |\n| 0.5        | 0.5              | 0.0            | 8.43027400970459        |\n| 0.5        | 0.0              | 0.5            | 7.512155532836914       |\n| 1/3        | 1/3              | 1/3            | 8.981038093566895       |\n","metadata":{}}]}