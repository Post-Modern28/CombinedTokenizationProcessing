{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token = \"YOUR_HUGGINGFACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM\n",
    "\n",
    "original_tokenizer = AutoTokenizer.from_pretrained(\"nickypro/tinyllama-110M\")\n",
    "model = LlamaForCausalLM.from_pretrained(\"nickypro/tinyllama-110M\")\n",
    "\n",
    "original_tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0224, -0.0145,  0.0013,  ...,  0.0682,  0.0056, -0.0279],\n",
       "        [-0.0008, -0.0003, -0.0180,  ...,  0.0243, -0.0092,  0.0170],\n",
       "        [-0.0224, -0.0145,  0.0013,  ...,  0.0681,  0.0056, -0.0279],\n",
       "        ...,\n",
       "        [-0.0224, -0.0145,  0.0013,  ...,  0.0682,  0.0056, -0.0279],\n",
       "        [-0.0224, -0.0145,  0.0013,  ...,  0.0681,  0.0056, -0.0279],\n",
       "        [-0.0224, -0.0145,  0.0013,  ...,  0.0682,  0.0056, -0.0279]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "import torch.nn as nn\n",
    "\n",
    "original_vocab_size, embedding_dim = model.model.embed_tokens.weight.shape\n",
    "\n",
    "extra_embedding_1 = nn.Embedding(original_vocab_size, embedding_dim)\n",
    "extra_embedding_2 = nn.Embedding(original_vocab_size, embedding_dim)\n",
    "\n",
    "nn.init.xavier_uniform_(extra_embedding_1.weight)\n",
    "nn.init.xavier_uniform_(extra_embedding_2.weight)\n",
    "\n",
    "extra_embedding_1.weight.requires_grad = True\n",
    "extra_embedding_2.weight.requires_grad = True\n",
    "\n",
    "# Add the new embeddings as attributes\n",
    "model.model.extra_embedding_1 = extra_embedding_1\n",
    "model.model.extra_embedding_2 = extra_embedding_2\n",
    "\n",
    "# Copy weights\n",
    "model.model.extra_embedding_1.weight.data.copy_(model.model.embed_tokens.weight.data)\n",
    "model.model.extra_embedding_2.weight.data.copy_(model.model.embed_tokens.weight.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Union\n",
    "from cachetools import Cache\n",
    "import types\n",
    "\n",
    "def modified_forward(\n",
    "    self,\n",
    "    input_ids: torch.LongTensor = None,\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    "    position_ids: Optional[torch.LongTensor] = None,\n",
    "    past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n",
    "    inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "    labels: Optional[torch.LongTensor] = None,\n",
    "    use_cache: Optional[bool] = None,\n",
    "    output_attentions: Optional[bool] = None,\n",
    "    output_hidden_states: Optional[bool] = None,\n",
    "    return_dict: Optional[bool] = None,\n",
    "    cache_position: Optional[torch.LongTensor] = None,\n",
    "    num_logits_to_keep: int = 0,\n",
    "    **kwargs\n",
    "):\n",
    "    if input_ids is None and inputs_embeds is None:\n",
    "        raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "    batch_size = input_ids.shape[0]\n",
    "    combined_embeds = []\n",
    "\n",
    "    for batch_idx in range(batch_size):\n",
    "        str_input_ids = \" \".join([str(i) for i in input_ids[batch_idx].tolist()])\n",
    "        input_ids_parts = str_input_ids.split(\" 200000 \")\n",
    "        # print(len(input_ids_parts), input_ids_parts)\n",
    "        bpe = torch.tensor([list(map(int, input_ids_parts[0].split(\" \")))], \n",
    "                           device=input_ids.device)\n",
    "        wordpiece = torch.tensor([list(map(int, input_ids_parts[1].split(\" \")))], \n",
    "                               device=input_ids.device)\n",
    "        unigram = torch.tensor([list(map(int, input_ids_parts[2].split(\" \")))], \n",
    "                                     device=input_ids.device)\n",
    "        \n",
    "        bpe_embedding = self.model.embed_tokens(bpe)\n",
    "        wordpiece_embedding = self.model.extra_embedding_1(wordpiece)\n",
    "        unigram_embedding = self.model.extra_embedding_2(unigram)\n",
    "        \n",
    "        # print(f\"Shapes: BPE {bpe_embedding.shape}, Unigram {unigram_embedding.shape}, SentencePiece {sentencepiece_embedding.shape}\")\n",
    "        \n",
    "        min_length = min(bpe_embedding.shape[1], wordpiece_embedding.shape[1], unigram_embedding.shape[1])\n",
    "        bpe_embedding = bpe_embedding[:, :min_length, :]\n",
    "        wordpiece_embedding = wordpiece_embedding[:, :min_length, :]\n",
    "        unigram_embedding = unigram_embedding[:, :min_length, :]\n",
    "        \n",
    "        batch_embeds = bpe_embedding + wordpiece_embedding + unigram_embedding\n",
    "        combined_embeds.append(batch_embeds)\n",
    "\n",
    "    inputs_embeds = torch.cat(combined_embeds, dim=0)\n",
    "    # print(f\"Final inputs_embeds shape: {inputs_embeds.shape}\")\n",
    "    \n",
    "    if attention_mask is not None:\n",
    "        attention_mask = attention_mask[:, :inputs_embeds.shape[1]]\n",
    "    \n",
    "    return self.original_forward(\n",
    "        input_ids=None,\n",
    "        attention_mask=attention_mask,\n",
    "        position_ids=position_ids,\n",
    "        past_key_values=past_key_values,\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        use_cache=use_cache,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict,\n",
    "        cache_position=cache_position,\n",
    "        labels=labels,\n",
    "        num_logits_to_keep=num_logits_to_keep,\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "model.original_forward = model.forward\n",
    "model.forward = types.MethodType(modified_forward, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze everething, except the new embeddings\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.model.extra_embedding_1.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.model.extra_embedding_2.parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight False\n",
      "model.layers.0.self_attn.q_proj.weight False\n",
      "model.layers.0.self_attn.k_proj.weight False\n",
      "model.layers.0.self_attn.v_proj.weight False\n",
      "model.layers.0.self_attn.o_proj.weight False\n",
      "model.layers.0.mlp.gate_proj.weight False\n",
      "model.layers.0.mlp.up_proj.weight False\n",
      "model.layers.0.mlp.down_proj.weight False\n",
      "model.layers.0.input_layernorm.weight False\n",
      "model.layers.0.post_attention_layernorm.weight False\n",
      "model.layers.1.self_attn.q_proj.weight False\n",
      "model.layers.1.self_attn.k_proj.weight False\n",
      "model.layers.1.self_attn.v_proj.weight False\n",
      "model.layers.1.self_attn.o_proj.weight False\n",
      "model.layers.1.mlp.gate_proj.weight False\n",
      "model.layers.1.mlp.up_proj.weight False\n",
      "model.layers.1.mlp.down_proj.weight False\n",
      "model.layers.1.input_layernorm.weight False\n",
      "model.layers.1.post_attention_layernorm.weight False\n",
      "model.layers.2.self_attn.q_proj.weight False\n",
      "model.layers.2.self_attn.k_proj.weight False\n",
      "model.layers.2.self_attn.v_proj.weight False\n",
      "model.layers.2.self_attn.o_proj.weight False\n",
      "model.layers.2.mlp.gate_proj.weight False\n",
      "model.layers.2.mlp.up_proj.weight False\n",
      "model.layers.2.mlp.down_proj.weight False\n",
      "model.layers.2.input_layernorm.weight False\n",
      "model.layers.2.post_attention_layernorm.weight False\n",
      "model.layers.3.self_attn.q_proj.weight False\n",
      "model.layers.3.self_attn.k_proj.weight False\n",
      "model.layers.3.self_attn.v_proj.weight False\n",
      "model.layers.3.self_attn.o_proj.weight False\n",
      "model.layers.3.mlp.gate_proj.weight False\n",
      "model.layers.3.mlp.up_proj.weight False\n",
      "model.layers.3.mlp.down_proj.weight False\n",
      "model.layers.3.input_layernorm.weight False\n",
      "model.layers.3.post_attention_layernorm.weight False\n",
      "model.layers.4.self_attn.q_proj.weight False\n",
      "model.layers.4.self_attn.k_proj.weight False\n",
      "model.layers.4.self_attn.v_proj.weight False\n",
      "model.layers.4.self_attn.o_proj.weight False\n",
      "model.layers.4.mlp.gate_proj.weight False\n",
      "model.layers.4.mlp.up_proj.weight False\n",
      "model.layers.4.mlp.down_proj.weight False\n",
      "model.layers.4.input_layernorm.weight False\n",
      "model.layers.4.post_attention_layernorm.weight False\n",
      "model.layers.5.self_attn.q_proj.weight False\n",
      "model.layers.5.self_attn.k_proj.weight False\n",
      "model.layers.5.self_attn.v_proj.weight False\n",
      "model.layers.5.self_attn.o_proj.weight False\n",
      "model.layers.5.mlp.gate_proj.weight False\n",
      "model.layers.5.mlp.up_proj.weight False\n",
      "model.layers.5.mlp.down_proj.weight False\n",
      "model.layers.5.input_layernorm.weight False\n",
      "model.layers.5.post_attention_layernorm.weight False\n",
      "model.layers.6.self_attn.q_proj.weight False\n",
      "model.layers.6.self_attn.k_proj.weight False\n",
      "model.layers.6.self_attn.v_proj.weight False\n",
      "model.layers.6.self_attn.o_proj.weight False\n",
      "model.layers.6.mlp.gate_proj.weight False\n",
      "model.layers.6.mlp.up_proj.weight False\n",
      "model.layers.6.mlp.down_proj.weight False\n",
      "model.layers.6.input_layernorm.weight False\n",
      "model.layers.6.post_attention_layernorm.weight False\n",
      "model.layers.7.self_attn.q_proj.weight False\n",
      "model.layers.7.self_attn.k_proj.weight False\n",
      "model.layers.7.self_attn.v_proj.weight False\n",
      "model.layers.7.self_attn.o_proj.weight False\n",
      "model.layers.7.mlp.gate_proj.weight False\n",
      "model.layers.7.mlp.up_proj.weight False\n",
      "model.layers.7.mlp.down_proj.weight False\n",
      "model.layers.7.input_layernorm.weight False\n",
      "model.layers.7.post_attention_layernorm.weight False\n",
      "model.layers.8.self_attn.q_proj.weight False\n",
      "model.layers.8.self_attn.k_proj.weight False\n",
      "model.layers.8.self_attn.v_proj.weight False\n",
      "model.layers.8.self_attn.o_proj.weight False\n",
      "model.layers.8.mlp.gate_proj.weight False\n",
      "model.layers.8.mlp.up_proj.weight False\n",
      "model.layers.8.mlp.down_proj.weight False\n",
      "model.layers.8.input_layernorm.weight False\n",
      "model.layers.8.post_attention_layernorm.weight False\n",
      "model.layers.9.self_attn.q_proj.weight False\n",
      "model.layers.9.self_attn.k_proj.weight False\n",
      "model.layers.9.self_attn.v_proj.weight False\n",
      "model.layers.9.self_attn.o_proj.weight False\n",
      "model.layers.9.mlp.gate_proj.weight False\n",
      "model.layers.9.mlp.up_proj.weight False\n",
      "model.layers.9.mlp.down_proj.weight False\n",
      "model.layers.9.input_layernorm.weight False\n",
      "model.layers.9.post_attention_layernorm.weight False\n",
      "model.layers.10.self_attn.q_proj.weight False\n",
      "model.layers.10.self_attn.k_proj.weight False\n",
      "model.layers.10.self_attn.v_proj.weight False\n",
      "model.layers.10.self_attn.o_proj.weight False\n",
      "model.layers.10.mlp.gate_proj.weight False\n",
      "model.layers.10.mlp.up_proj.weight False\n",
      "model.layers.10.mlp.down_proj.weight False\n",
      "model.layers.10.input_layernorm.weight False\n",
      "model.layers.10.post_attention_layernorm.weight False\n",
      "model.layers.11.self_attn.q_proj.weight False\n",
      "model.layers.11.self_attn.k_proj.weight False\n",
      "model.layers.11.self_attn.v_proj.weight False\n",
      "model.layers.11.self_attn.o_proj.weight False\n",
      "model.layers.11.mlp.gate_proj.weight False\n",
      "model.layers.11.mlp.up_proj.weight False\n",
      "model.layers.11.mlp.down_proj.weight False\n",
      "model.layers.11.input_layernorm.weight False\n",
      "model.layers.11.post_attention_layernorm.weight False\n",
      "model.norm.weight False\n",
      "model.extra_embedding_1.weight True\n",
      "model.extra_embedding_2.weight True\n"
     ]
    }
   ],
   "source": [
    "#check if the embeddings are trainable\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"mlabonne/FineTome-Alpaca-100k\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(examples, tokenizer):\n",
    "    texts = [f\"### Instruction: {instruction}\\n### Response: {output}\" \n",
    "             for instruction, output in zip(examples['instruction'], examples['output'])]\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None\n",
    "    )\n",
    "        \n",
    "    # Add labels for causal language modeling\n",
    "    tokenized[\"labels\"] = [ids.copy() for ids in tokenized[\"input_ids\"]]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "wordpiece_tokenizer = AutoTokenizer.from_pretrained(\"trained_tokenizers/wordpiece\")\n",
    "unigram_tokenizer = AutoTokenizer.from_pretrained(\"trained_tokenizers/unigram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Can you tell me a joke?\"\n",
    "\n",
    "original_tokenizer.pad_token_id = original_tokenizer.eos_token_id\n",
    "unigram_tokenizer.pad_token_id = unigram_tokenizer.eos_token_id\n",
    "wordpiece_tokenizer.pad_token_id = wordpiece_tokenizer.eos_token_id\n",
    "\n",
    "original_tokens = original_tokenizer(text, padding='max_length', max_length=10)\n",
    "wordpiece_tokens = wordpiece_tokenizer(text, padding='max_length', max_length=10)\n",
    "unigram_tokens = unigram_tokenizer(text, padding='max_length', max_length=10)\n",
    "\n",
    "combined_text = original_tokens[\"input_ids\"] + [200000] + wordpiece_tokens[\"input_ids\"] + [200000] + unigram_tokens[\"input_ids\"]\n",
    "combined_attention_mask = original_tokens['attention_mask'] + [200000] + wordpiece_tokens['attention_mask'] + [200000] + unigram_tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((768,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((768,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((768,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "    (extra_embedding_1): Embedding(32000, 768)\n",
       "    (extra_embedding_2): Embedding(32000, 768)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "import gc\n",
    "\n",
    "\n",
    "original_tokenizer.pad_token = original_tokenizer.eos_token\n",
    "wordpiece_tokenizer.pad_token = wordpiece_tokenizer.eos_token\n",
    "unigram_tokenizer.pad_token = unigram_tokenizer.eos_token\n",
    "\n",
    "orig_tokenized_text = ds.map(lambda examples: tokenize(examples, original_tokenizer), batched=True, remove_columns=['instruction',\"source\",\"score\",'output'])\n",
    "orig_tokenized_text = orig_tokenized_text.rename_column(\"input_ids\", \"input_ids1\")\n",
    "orig_tokenized_text = orig_tokenized_text.rename_column(\"attention_mask\", \"attention_mask1\")\n",
    "orig_tokenized_text = orig_tokenized_text.rename_column(\"labels\", \"labels1\")\n",
    "\n",
    "wordpiece_tokenized_text = ds.map(lambda examples: tokenize(examples, wordpiece_tokenizer), batched=True, remove_columns=['instruction',\"source\",\"score\",'output'])\n",
    "wordpiece_tokenized_text = wordpiece_tokenized_text.rename_column(\"input_ids\", \"input_ids2\")\n",
    "wordpiece_tokenized_text = wordpiece_tokenized_text.rename_column(\"attention_mask\", \"attention_mask2\")\n",
    "wordpiece_tokenized_text = wordpiece_tokenized_text.rename_column(\"labels\", \"labels2\")\n",
    "\n",
    "unigram_tokenized_text = ds.map(lambda examples: tokenize(examples, unigram_tokenizer), batched=True, remove_columns=['instruction',\"source\",\"score\",'output'])\n",
    "unigram_tokenized_text = unigram_tokenized_text.rename_column(\"input_ids\", \"input_ids3\")\n",
    "unigram_tokenized_text = unigram_tokenized_text.rename_column(\"attention_mask\", \"attention_mask3\")\n",
    "unigram_tokenized_text = unigram_tokenized_text.rename_column(\"labels\", \"labels3\")\n",
    "\n",
    "# Unify the tokenized datasets\n",
    "wordpiece_tokenized_text = wordpiece_tokenized_text.remove_columns(['token_type_ids'])\n",
    "unigram_tokenized_text = unigram_tokenized_text.remove_columns(['token_type_ids'])\n",
    "\n",
    "# Unify the tokenized datasets\n",
    "unified_tokenized_text = datasets.concatenate_datasets([orig_tokenized_text, wordpiece_tokenized_text, unigram_tokenized_text], axis=1)\n",
    "\n",
    "\n",
    "del orig_tokenized_text\n",
    "del wordpiece_tokenized_text\n",
    "del unigram_tokenized_text\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_rows(row, max_length=1024):\n",
    "    sep = [200000]\n",
    "\n",
    "    combined_input_ids = row['input_ids1'] + sep + row['input_ids2'] + sep + row['input_ids3']\n",
    "    combined_attention_mask = row['attention_mask1'] + sep + row['attention_mask2'] + sep + row['attention_mask3']\n",
    "    combined_labels = row['labels1'] \n",
    "\n",
    "    return {\n",
    "        'input_ids': combined_input_ids,\n",
    "        'attention_mask': combined_attention_mask,\n",
    "        'labels': combined_labels\n",
    "    }\n",
    "\n",
    "unified_tokenized_text = unified_tokenized_text.map(\n",
    "    lambda row: concat_rows(row, max_length=1024),\n",
    "    remove_columns=['input_ids1', 'input_ids2', 'input_ids3', 'attention_mask1', 'attention_mask2', 'attention_mask3', 'labels1', 'labels2', 'labels3']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3074\n",
      "1024\n",
      "3074\n"
     ]
    }
   ],
   "source": [
    "print(len(unified_tokenized_text[0]['input_ids']))\n",
    "print(len(unified_tokenized_text[0]['labels']))\n",
    "print(len(unified_tokenized_text[0]['attention_mask']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yarab\\AppData\\Local\\Temp\\ipykernel_40612\\915514245.py:58: FutureWarning: `num_logits_to_keep` is deprecated and will be removed in version 4.50 for `LlamaForCausalLM.forward`. Use `logits_to_keep` instead.\n",
      "  return self.original_forward(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150000' max='150000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150000/150000 5:33:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>8.939300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>8.575700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>8.465400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>8.387700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>8.361700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>8.314300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>8.317200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>8.276100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>8.237100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>8.210900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>8.208100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>8.258100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>8.198100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>8.196900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>8.177700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>8.207000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>8.161700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>8.175200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>8.129100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>8.159100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>8.181100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>8.131100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>8.103100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>8.149000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>8.174600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>8.111400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>8.115500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>8.153200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>8.105900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>8.125200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>8.156700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>8.141800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>8.091700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>8.118900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>8.100300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>8.102400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>8.121000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>8.098100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>8.113700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>8.094500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>8.096800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>8.080500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>8.086700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>8.094100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>8.105900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>8.067000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>8.075200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>8.055100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>8.104400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>8.071600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>8.054800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>8.065000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>8.029300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>8.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>8.020800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>8.079600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>8.070700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>8.061700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>8.074000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>8.092000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>8.066400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>8.065300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>8.080300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>8.034500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>8.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>8.036400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>8.057500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>8.077400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>8.013300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>8.039400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71000</td>\n",
       "      <td>8.062700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>8.051000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73000</td>\n",
       "      <td>8.010500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>8.048500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>8.047800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>8.099400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77000</td>\n",
       "      <td>8.074000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>8.041300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79000</td>\n",
       "      <td>8.035100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>8.056600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81000</td>\n",
       "      <td>8.053800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>8.059600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83000</td>\n",
       "      <td>8.034900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>8.074900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85000</td>\n",
       "      <td>8.030500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>8.036500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87000</td>\n",
       "      <td>8.069300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>8.034300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89000</td>\n",
       "      <td>8.061800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>8.073500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91000</td>\n",
       "      <td>8.019300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>8.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93000</td>\n",
       "      <td>8.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>8.028900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95000</td>\n",
       "      <td>8.060800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>8.014100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97000</td>\n",
       "      <td>8.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98000</td>\n",
       "      <td>8.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99000</td>\n",
       "      <td>8.005100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>8.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101000</td>\n",
       "      <td>8.052300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102000</td>\n",
       "      <td>8.019500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103000</td>\n",
       "      <td>8.021300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104000</td>\n",
       "      <td>8.032600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105000</td>\n",
       "      <td>8.035500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106000</td>\n",
       "      <td>8.040700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107000</td>\n",
       "      <td>8.012900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108000</td>\n",
       "      <td>8.014200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109000</td>\n",
       "      <td>8.049000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>7.998900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111000</td>\n",
       "      <td>8.029100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112000</td>\n",
       "      <td>8.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113000</td>\n",
       "      <td>8.019400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114000</td>\n",
       "      <td>8.015100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115000</td>\n",
       "      <td>8.048900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116000</td>\n",
       "      <td>7.994900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117000</td>\n",
       "      <td>8.026200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118000</td>\n",
       "      <td>8.017000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119000</td>\n",
       "      <td>7.967200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>8.060500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121000</td>\n",
       "      <td>8.030800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122000</td>\n",
       "      <td>8.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123000</td>\n",
       "      <td>7.984400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124000</td>\n",
       "      <td>7.969400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125000</td>\n",
       "      <td>8.024700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126000</td>\n",
       "      <td>8.011100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127000</td>\n",
       "      <td>8.033700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128000</td>\n",
       "      <td>8.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129000</td>\n",
       "      <td>7.997300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130000</td>\n",
       "      <td>8.032400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131000</td>\n",
       "      <td>8.033900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132000</td>\n",
       "      <td>8.033300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133000</td>\n",
       "      <td>8.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134000</td>\n",
       "      <td>8.025900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135000</td>\n",
       "      <td>8.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136000</td>\n",
       "      <td>8.047100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137000</td>\n",
       "      <td>7.994100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138000</td>\n",
       "      <td>8.015600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139000</td>\n",
       "      <td>7.976200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140000</td>\n",
       "      <td>8.019700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141000</td>\n",
       "      <td>8.042600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142000</td>\n",
       "      <td>8.028700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143000</td>\n",
       "      <td>7.996500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144000</td>\n",
       "      <td>7.983500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145000</td>\n",
       "      <td>8.025600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146000</td>\n",
       "      <td>8.016900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147000</td>\n",
       "      <td>8.046300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148000</td>\n",
       "      <td>8.017800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149000</td>\n",
       "      <td>8.019200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150000</td>\n",
       "      <td>8.048700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yarab\\AppData\\Local\\Temp\\ipykernel_40612\\915514245.py:58: FutureWarning: `num_logits_to_keep` is deprecated and will be removed in version 4.50 for `LlamaForCausalLM.forward`. Use `logits_to_keep` instead.\n",
      "  return self.original_forward(\n",
      "C:\\Users\\yarab\\AppData\\Local\\Temp\\ipykernel_40612\\915514245.py:58: FutureWarning: `num_logits_to_keep` is deprecated and will be removed in version 4.50 for `LlamaForCausalLM.forward`. Use `logits_to_keep` instead.\n",
      "  return self.original_forward(\n",
      "C:\\Users\\yarab\\AppData\\Local\\Temp\\ipykernel_40612\\915514245.py:58: FutureWarning: `num_logits_to_keep` is deprecated and will be removed in version 4.50 for `LlamaForCausalLM.forward`. Use `logits_to_keep` instead.\n",
      "  return self.original_forward(\n",
      "C:\\Users\\yarab\\AppData\\Local\\Temp\\ipykernel_40612\\915514245.py:58: FutureWarning: `num_logits_to_keep` is deprecated and will be removed in version 4.50 for `LlamaForCausalLM.forward`. Use `logits_to_keep` instead.\n",
      "  return self.original_forward(\n",
      "C:\\Users\\yarab\\AppData\\Local\\Temp\\ipykernel_40612\\915514245.py:58: FutureWarning: `num_logits_to_keep` is deprecated and will be removed in version 4.50 for `LlamaForCausalLM.forward`. Use `logits_to_keep` instead.\n",
      "  return self.original_forward(\n",
      "C:\\Users\\yarab\\AppData\\Local\\Temp\\ipykernel_40612\\915514245.py:58: FutureWarning: `num_logits_to_keep` is deprecated and will be removed in version 4.50 for `LlamaForCausalLM.forward`. Use `logits_to_keep` instead.\n",
      "  return self.original_forward(\n",
      "C:\\Users\\yarab\\AppData\\Local\\Temp\\ipykernel_40612\\915514245.py:58: FutureWarning: `num_logits_to_keep` is deprecated and will be removed in version 4.50 for `LlamaForCausalLM.forward`. Use `logits_to_keep` instead.\n",
      "  return self.original_forward(\n",
      "C:\\Users\\yarab\\AppData\\Local\\Temp\\ipykernel_40612\\915514245.py:58: FutureWarning: `num_logits_to_keep` is deprecated and will be removed in version 4.50 for `LlamaForCausalLM.forward`. Use `logits_to_keep` instead.\n",
      "  return self.original_forward(\n",
      "C:\\Users\\yarab\\AppData\\Local\\Temp\\ipykernel_40612\\915514245.py:58: FutureWarning: `num_logits_to_keep` is deprecated and will be removed in version 4.50 for `LlamaForCausalLM.forward`. Use `logits_to_keep` instead.\n",
      "  return self.original_forward(\n",
      "C:\\Users\\yarab\\AppData\\Local\\Temp\\ipykernel_40612\\915514245.py:58: FutureWarning: `num_logits_to_keep` is deprecated and will be removed in version 4.50 for `LlamaForCausalLM.forward`. Use `logits_to_keep` instead.\n",
      "  return self.original_forward(\n",
      "C:\\Users\\yarab\\AppData\\Local\\Temp\\ipykernel_40612\\915514245.py:58: FutureWarning: `num_logits_to_keep` is deprecated and will be removed in version 4.50 for `LlamaForCausalLM.forward`. Use `logits_to_keep` instead.\n",
      "  return self.original_forward(\n",
      "C:\\Users\\yarab\\AppData\\Local\\Temp\\ipykernel_40612\\915514245.py:58: FutureWarning: `num_logits_to_keep` is deprecated and will be removed in version 4.50 for `LlamaForCausalLM.forward`. Use `logits_to_keep` instead.\n",
      "  return self.original_forward(\n",
      "C:\\Users\\yarab\\AppData\\Local\\Temp\\ipykernel_40612\\915514245.py:58: FutureWarning: `num_logits_to_keep` is deprecated and will be removed in version 4.50 for `LlamaForCausalLM.forward`. Use `logits_to_keep` instead.\n",
      "  return self.original_forward(\n",
      "C:\\Users\\yarab\\AppData\\Local\\Temp\\ipykernel_40612\\915514245.py:58: FutureWarning: `num_logits_to_keep` is deprecated and will be removed in version 4.50 for `LlamaForCausalLM.forward`. Use `logits_to_keep` instead.\n",
      "  return self.original_forward(\n",
      "C:\\Users\\yarab\\AppData\\Local\\Temp\\ipykernel_40612\\915514245.py:58: FutureWarning: `num_logits_to_keep` is deprecated and will be removed in version 4.50 for `LlamaForCausalLM.forward`. Use `logits_to_keep` instead.\n",
      "  return self.original_forward(\n",
      "C:\\Users\\yarab\\AppData\\Local\\Temp\\ipykernel_40612\\915514245.py:58: FutureWarning: `num_logits_to_keep` is deprecated and will be removed in version 4.50 for `LlamaForCausalLM.forward`. Use `logits_to_keep` instead.\n",
      "  return self.original_forward(\n",
      "C:\\Users\\yarab\\AppData\\Local\\Temp\\ipykernel_40612\\915514245.py:58: FutureWarning: `num_logits_to_keep` is deprecated and will be removed in version 4.50 for `LlamaForCausalLM.forward`. Use `logits_to_keep` instead.\n",
      "  return self.original_forward(\n",
      "C:\\Users\\yarab\\AppData\\Local\\Temp\\ipykernel_40612\\915514245.py:58: FutureWarning: `num_logits_to_keep` is deprecated and will be removed in version 4.50 for `LlamaForCausalLM.forward`. Use `logits_to_keep` instead.\n",
      "  return self.original_forward(\n",
      "C:\\Users\\yarab\\AppData\\Local\\Temp\\ipykernel_40612\\915514245.py:58: FutureWarning: `num_logits_to_keep` is deprecated and will be removed in version 4.50 for `LlamaForCausalLM.forward`. Use `logits_to_keep` instead.\n",
      "  return self.original_forward(\n",
      "C:\\Users\\yarab\\AppData\\Local\\Temp\\ipykernel_40612\\915514245.py:58: FutureWarning: `num_logits_to_keep` is deprecated and will be removed in version 4.50 for `LlamaForCausalLM.forward`. Use `logits_to_keep` instead.\n",
      "  return self.original_forward(\n",
      "C:\\Users\\yarab\\AppData\\Local\\Temp\\ipykernel_40612\\915514245.py:58: FutureWarning: `num_logits_to_keep` is deprecated and will be removed in version 4.50 for `LlamaForCausalLM.forward`. Use `logits_to_keep` instead.\n",
      "  return self.original_forward(\n",
      "C:\\Users\\yarab\\AppData\\Local\\Temp\\ipykernel_40612\\915514245.py:58: FutureWarning: `num_logits_to_keep` is deprecated and will be removed in version 4.50 for `LlamaForCausalLM.forward`. Use `logits_to_keep` instead.\n",
      "  return self.original_forward(\n",
      "C:\\Users\\yarab\\AppData\\Local\\Temp\\ipykernel_40612\\915514245.py:58: FutureWarning: `num_logits_to_keep` is deprecated and will be removed in version 4.50 for `LlamaForCausalLM.forward`. Use `logits_to_keep` instead.\n",
      "  return self.original_forward(\n",
      "C:\\Users\\yarab\\AppData\\Local\\Temp\\ipykernel_40612\\915514245.py:58: FutureWarning: `num_logits_to_keep` is deprecated and will be removed in version 4.50 for `LlamaForCausalLM.forward`. Use `logits_to_keep` instead.\n",
      "  return self.original_forward(\n",
      "C:\\Users\\yarab\\AppData\\Local\\Temp\\ipykernel_40612\\915514245.py:58: FutureWarning: `num_logits_to_keep` is deprecated and will be removed in version 4.50 for `LlamaForCausalLM.forward`. Use `logits_to_keep` instead.\n",
      "  return self.original_forward(\n",
      "C:\\Users\\yarab\\AppData\\Local\\Temp\\ipykernel_40612\\915514245.py:58: FutureWarning: `num_logits_to_keep` is deprecated and will be removed in version 4.50 for `LlamaForCausalLM.forward`. Use `logits_to_keep` instead.\n",
      "  return self.original_forward(\n",
      "C:\\Users\\yarab\\AppData\\Local\\Temp\\ipykernel_40612\\915514245.py:58: FutureWarning: `num_logits_to_keep` is deprecated and will be removed in version 4.50 for `LlamaForCausalLM.forward`. Use `logits_to_keep` instead.\n",
      "  return self.original_forward(\n",
      "C:\\Users\\yarab\\AppData\\Local\\Temp\\ipykernel_40612\\915514245.py:58: FutureWarning: `num_logits_to_keep` is deprecated and will be removed in version 4.50 for `LlamaForCausalLM.forward`. Use `logits_to_keep` instead.\n",
      "  return self.original_forward(\n",
      "C:\\Users\\yarab\\AppData\\Local\\Temp\\ipykernel_40612\\915514245.py:58: FutureWarning: `num_logits_to_keep` is deprecated and will be removed in version 4.50 for `LlamaForCausalLM.forward`. Use `logits_to_keep` instead.\n",
      "  return self.original_forward(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=150000, training_loss=8.084710865885416, metrics={'train_runtime': 19986.5399, 'train_samples_per_second': 15.01, 'train_steps_per_second': 7.505, 'total_flos': 4.700666760192e+17, 'train_loss': 8.084710865885416, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "model.train()\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "        {\"params\": model.model.extra_embedding_1.parameters(), \"lr\": 2e-5},\n",
    "        {\"params\": model.model.extra_embedding_2.parameters(), \"lr\": 2e-5}\n",
    "])\n",
    "\n",
    "def multi_tokenizer_data_collator(features):\n",
    "    \"\"\"\n",
    "    Custom data collator for handling input from multiple tokenizers\n",
    "    with separator tokens (200000) preserved.\n",
    "    \"\"\"\n",
    "    # Input IDs with batch dimension\n",
    "    input_ids = torch.stack([torch.tensor(f[\"input_ids\"]) for f in features])\n",
    "    attention_mask = torch.stack([torch.tensor(f[\"attention_mask\"]) for f in features])\n",
    "    labels = torch.stack([torch.tensor(f[\"labels\"]) for f in features])\n",
    "\n",
    "    # print(input_ids.shape, labels.shape, attention_mask.shape)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,  \n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "# Create the data collator\n",
    "data_collator = multi_tokenizer_data_collator\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./trained_embeddings\",\n",
    "    per_device_train_batch_size=2,  \n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=5000,\n",
    "    learning_rate=2e-5,\n",
    "    remove_unused_columns=False,  \n",
    "    fp16=True,  \n",
    "    optim=\"adamw_torch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1000,  # Log every 100 steps\n",
    "    disable_tqdm=False,  # Ensure tqdm is enabled\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=unified_tokenized_text,\n",
    "    data_collator=data_collator,\n",
    "    optimizers=(optimizer, None)  \n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(\"saved_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
