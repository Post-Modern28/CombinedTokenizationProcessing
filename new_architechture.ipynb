{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token = \"YOUR_TOKEN\") # Replace YOUR_TOKEN with your Hugging Face token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM\n",
    "\n",
    "original_tokenizer = AutoTokenizer.from_pretrained(\"nickypro/tinyllama-110M\")\n",
    "model = LlamaForCausalLM.from_pretrained(\"nickypro/tinyllama-110M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0224, -0.0145,  0.0013,  ...,  0.0682,  0.0056, -0.0279],\n",
       "        [-0.0008, -0.0003, -0.0180,  ...,  0.0243, -0.0092,  0.0170],\n",
       "        [-0.0224, -0.0145,  0.0013,  ...,  0.0681,  0.0056, -0.0279],\n",
       "        ...,\n",
       "        [-0.0224, -0.0145,  0.0013,  ...,  0.0682,  0.0056, -0.0279],\n",
       "        [-0.0224, -0.0145,  0.0013,  ...,  0.0681,  0.0056, -0.0279],\n",
       "        [-0.0224, -0.0145,  0.0013,  ...,  0.0682,  0.0056, -0.0279]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "import torch.nn as nn\n",
    "\n",
    "original_vocab_size, embedding_dim = model.model.embed_tokens.weight.shape\n",
    "\n",
    "extra_embedding_1 = nn.Embedding(original_vocab_size, embedding_dim)\n",
    "extra_embedding_2 = nn.Embedding(original_vocab_size, embedding_dim)\n",
    "\n",
    "nn.init.xavier_uniform_(extra_embedding_1.weight)\n",
    "nn.init.xavier_uniform_(extra_embedding_2.weight)\n",
    "\n",
    "extra_embedding_1.weight.requires_grad = True\n",
    "extra_embedding_2.weight.requires_grad = True\n",
    "\n",
    "# Add the new embeddings as attributes\n",
    "model.model.extra_embedding_1 = extra_embedding_1\n",
    "model.model.extra_embedding_2 = extra_embedding_2\n",
    "\n",
    "# Copy weights\n",
    "model.model.extra_embedding_1.weight.data.copy_(model.model.embed_tokens.weight.data)\n",
    "model.model.extra_embedding_2.weight.data.copy_(model.model.embed_tokens.weight.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Union\n",
    "from cachetools import Cache\n",
    "import types\n",
    "\n",
    "def modified_forward(\n",
    "    self,\n",
    "    input_ids: torch.LongTensor = None,\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    "    position_ids: Optional[torch.LongTensor] = None,\n",
    "    past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n",
    "    inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "    labels: Optional[torch.LongTensor] = None,\n",
    "    use_cache: Optional[bool] = None,\n",
    "    output_attentions: Optional[bool] = None,\n",
    "    output_hidden_states: Optional[bool] = None,\n",
    "    return_dict: Optional[bool] = None,\n",
    "    cache_position: Optional[torch.LongTensor] = None,\n",
    "    num_logits_to_keep: int = 0,\n",
    "    **kwargs\n",
    "):\n",
    "    if input_ids is None and inputs_embeds is None:\n",
    "        raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "    batch_size = input_ids.shape[0]\n",
    "    combined_embeds = []\n",
    "\n",
    "    for batch_idx in range(batch_size):\n",
    "        str_input_ids = \" \".join([str(i) for i in input_ids[batch_idx].tolist()])\n",
    "        input_ids_parts = str_input_ids.split(\" 200000 \")\n",
    "        # print(len(input_ids_parts), input_ids_parts)\n",
    "        bpe = torch.tensor([list(map(int, input_ids_parts[0].split(\" \")))], \n",
    "                           device=input_ids.device)\n",
    "        unigram = torch.tensor([list(map(int, input_ids_parts[1].split(\" \")))], \n",
    "                               device=input_ids.device)\n",
    "        sentencepiece = torch.tensor([list(map(int, input_ids_parts[2].split(\" \")))], \n",
    "                                     device=input_ids.device)\n",
    "        \n",
    "        bpe_embedding = self.model.embed_tokens(bpe)\n",
    "        unigram_embedding = self.model.extra_embedding_1(unigram)\n",
    "        sentencepiece_embedding = self.model.extra_embedding_2(sentencepiece)\n",
    "        \n",
    "        # print(f\"Shapes: BPE {bpe_embedding.shape}, Unigram {unigram_embedding.shape}, SentencePiece {sentencepiece_embedding.shape}\")\n",
    "        \n",
    "        min_length = min(bpe_embedding.shape[1], unigram_embedding.shape[1], sentencepiece_embedding.shape[1])\n",
    "        bpe_embedding = bpe_embedding[:, :min_length, :]\n",
    "        unigram_embedding = unigram_embedding[:, :min_length, :]\n",
    "        sentencepiece_embedding = sentencepiece_embedding[:, :min_length, :]\n",
    "        \n",
    "        batch_embeds = bpe_embedding + unigram_embedding + sentencepiece_embedding\n",
    "        combined_embeds.append(batch_embeds)\n",
    "\n",
    "    inputs_embeds = torch.cat(combined_embeds, dim=0)\n",
    "    # print(f\"Final inputs_embeds shape: {inputs_embeds.shape}\")\n",
    "    \n",
    "    if attention_mask is not None:\n",
    "        attention_mask = attention_mask[:, :inputs_embeds.shape[1]]\n",
    "    \n",
    "    return self.original_forward(\n",
    "        input_ids=None,\n",
    "        attention_mask=attention_mask,\n",
    "        position_ids=position_ids,\n",
    "        past_key_values=past_key_values,\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        use_cache=use_cache,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict,\n",
    "        cache_position=cache_position,\n",
    "        labels=labels,\n",
    "        num_logits_to_keep=num_logits_to_keep,\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "model.original_forward = model.forward\n",
    "model.forward = types.MethodType(modified_forward, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze everething, except the new embeddings\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.model.extra_embedding_1.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.model.extra_embedding_2.parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight False\n",
      "model.layers.0.self_attn.q_proj.weight False\n",
      "model.layers.0.self_attn.k_proj.weight False\n",
      "model.layers.0.self_attn.v_proj.weight False\n",
      "model.layers.0.self_attn.o_proj.weight False\n",
      "model.layers.0.mlp.gate_proj.weight False\n",
      "model.layers.0.mlp.up_proj.weight False\n",
      "model.layers.0.mlp.down_proj.weight False\n",
      "model.layers.0.input_layernorm.weight False\n",
      "model.layers.0.post_attention_layernorm.weight False\n",
      "model.layers.1.self_attn.q_proj.weight False\n",
      "model.layers.1.self_attn.k_proj.weight False\n",
      "model.layers.1.self_attn.v_proj.weight False\n",
      "model.layers.1.self_attn.o_proj.weight False\n",
      "model.layers.1.mlp.gate_proj.weight False\n",
      "model.layers.1.mlp.up_proj.weight False\n",
      "model.layers.1.mlp.down_proj.weight False\n",
      "model.layers.1.input_layernorm.weight False\n",
      "model.layers.1.post_attention_layernorm.weight False\n",
      "model.layers.2.self_attn.q_proj.weight False\n",
      "model.layers.2.self_attn.k_proj.weight False\n",
      "model.layers.2.self_attn.v_proj.weight False\n",
      "model.layers.2.self_attn.o_proj.weight False\n",
      "model.layers.2.mlp.gate_proj.weight False\n",
      "model.layers.2.mlp.up_proj.weight False\n",
      "model.layers.2.mlp.down_proj.weight False\n",
      "model.layers.2.input_layernorm.weight False\n",
      "model.layers.2.post_attention_layernorm.weight False\n",
      "model.layers.3.self_attn.q_proj.weight False\n",
      "model.layers.3.self_attn.k_proj.weight False\n",
      "model.layers.3.self_attn.v_proj.weight False\n",
      "model.layers.3.self_attn.o_proj.weight False\n",
      "model.layers.3.mlp.gate_proj.weight False\n",
      "model.layers.3.mlp.up_proj.weight False\n",
      "model.layers.3.mlp.down_proj.weight False\n",
      "model.layers.3.input_layernorm.weight False\n",
      "model.layers.3.post_attention_layernorm.weight False\n",
      "model.layers.4.self_attn.q_proj.weight False\n",
      "model.layers.4.self_attn.k_proj.weight False\n",
      "model.layers.4.self_attn.v_proj.weight False\n",
      "model.layers.4.self_attn.o_proj.weight False\n",
      "model.layers.4.mlp.gate_proj.weight False\n",
      "model.layers.4.mlp.up_proj.weight False\n",
      "model.layers.4.mlp.down_proj.weight False\n",
      "model.layers.4.input_layernorm.weight False\n",
      "model.layers.4.post_attention_layernorm.weight False\n",
      "model.layers.5.self_attn.q_proj.weight False\n",
      "model.layers.5.self_attn.k_proj.weight False\n",
      "model.layers.5.self_attn.v_proj.weight False\n",
      "model.layers.5.self_attn.o_proj.weight False\n",
      "model.layers.5.mlp.gate_proj.weight False\n",
      "model.layers.5.mlp.up_proj.weight False\n",
      "model.layers.5.mlp.down_proj.weight False\n",
      "model.layers.5.input_layernorm.weight False\n",
      "model.layers.5.post_attention_layernorm.weight False\n",
      "model.layers.6.self_attn.q_proj.weight False\n",
      "model.layers.6.self_attn.k_proj.weight False\n",
      "model.layers.6.self_attn.v_proj.weight False\n",
      "model.layers.6.self_attn.o_proj.weight False\n",
      "model.layers.6.mlp.gate_proj.weight False\n",
      "model.layers.6.mlp.up_proj.weight False\n",
      "model.layers.6.mlp.down_proj.weight False\n",
      "model.layers.6.input_layernorm.weight False\n",
      "model.layers.6.post_attention_layernorm.weight False\n",
      "model.layers.7.self_attn.q_proj.weight False\n",
      "model.layers.7.self_attn.k_proj.weight False\n",
      "model.layers.7.self_attn.v_proj.weight False\n",
      "model.layers.7.self_attn.o_proj.weight False\n",
      "model.layers.7.mlp.gate_proj.weight False\n",
      "model.layers.7.mlp.up_proj.weight False\n",
      "model.layers.7.mlp.down_proj.weight False\n",
      "model.layers.7.input_layernorm.weight False\n",
      "model.layers.7.post_attention_layernorm.weight False\n",
      "model.layers.8.self_attn.q_proj.weight False\n",
      "model.layers.8.self_attn.k_proj.weight False\n",
      "model.layers.8.self_attn.v_proj.weight False\n",
      "model.layers.8.self_attn.o_proj.weight False\n",
      "model.layers.8.mlp.gate_proj.weight False\n",
      "model.layers.8.mlp.up_proj.weight False\n",
      "model.layers.8.mlp.down_proj.weight False\n",
      "model.layers.8.input_layernorm.weight False\n",
      "model.layers.8.post_attention_layernorm.weight False\n",
      "model.layers.9.self_attn.q_proj.weight False\n",
      "model.layers.9.self_attn.k_proj.weight False\n",
      "model.layers.9.self_attn.v_proj.weight False\n",
      "model.layers.9.self_attn.o_proj.weight False\n",
      "model.layers.9.mlp.gate_proj.weight False\n",
      "model.layers.9.mlp.up_proj.weight False\n",
      "model.layers.9.mlp.down_proj.weight False\n",
      "model.layers.9.input_layernorm.weight False\n",
      "model.layers.9.post_attention_layernorm.weight False\n",
      "model.layers.10.self_attn.q_proj.weight False\n",
      "model.layers.10.self_attn.k_proj.weight False\n",
      "model.layers.10.self_attn.v_proj.weight False\n",
      "model.layers.10.self_attn.o_proj.weight False\n",
      "model.layers.10.mlp.gate_proj.weight False\n",
      "model.layers.10.mlp.up_proj.weight False\n",
      "model.layers.10.mlp.down_proj.weight False\n",
      "model.layers.10.input_layernorm.weight False\n",
      "model.layers.10.post_attention_layernorm.weight False\n",
      "model.layers.11.self_attn.q_proj.weight False\n",
      "model.layers.11.self_attn.k_proj.weight False\n",
      "model.layers.11.self_attn.v_proj.weight False\n",
      "model.layers.11.self_attn.o_proj.weight False\n",
      "model.layers.11.mlp.gate_proj.weight False\n",
      "model.layers.11.mlp.up_proj.weight False\n",
      "model.layers.11.mlp.down_proj.weight False\n",
      "model.layers.11.input_layernorm.weight False\n",
      "model.layers.11.post_attention_layernorm.weight False\n",
      "model.norm.weight False\n",
      "model.extra_embedding_1.weight True\n",
      "model.extra_embedding_2.weight True\n"
     ]
    }
   ],
   "source": [
    "#check if the embeddings are trainable\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"mlabonne/FineTome-Alpaca-100k\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(examples, tokenizer):\n",
    "    texts = [f\"### Instruction: {instruction}\\n### Response: {output}\" \n",
    "             for instruction, output in zip(examples['instruction'], examples['output'])]\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None\n",
    "    )\n",
    "        \n",
    "    # Add labels for causal language modeling\n",
    "    tokenized[\"labels\"] = [ids.copy() for ids in tokenized[\"input_ids\"]]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "wordpiece_tokenizer = AutoTokenizer.from_pretrained(\"wordpiece-small\")\n",
    "unigram_tokenizer = AutoTokenizer.from_pretrained(\"wordpiece-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "import gc\n",
    "\n",
    "\n",
    "original_tokenizer.pad_token = original_tokenizer.eos_token\n",
    "wordpiece_tokenizer.pad_token = wordpiece_tokenizer.eos_token\n",
    "unigram_tokenizer.pad_token = unigram_tokenizer.eos_token\n",
    "\n",
    "orig_tokenized_text = ds.map(lambda examples: tokenize(examples, original_tokenizer), batched=True, remove_columns=['instruction',\"source\",\"score\",'output'])\n",
    "orig_tokenized_text = orig_tokenized_text.rename_column(\"input_ids\", \"input_ids1\")\n",
    "orig_tokenized_text = orig_tokenized_text.rename_column(\"attention_mask\", \"attention_mask1\")\n",
    "orig_tokenized_text = orig_tokenized_text.rename_column(\"labels\", \"labels1\")\n",
    "\n",
    "wordpiece_tokenized_text = ds.map(lambda examples: tokenize(examples, wordpiece_tokenizer), batched=True, remove_columns=['instruction',\"source\",\"score\",'output'])\n",
    "wordpiece_tokenized_text = wordpiece_tokenized_text.rename_column(\"input_ids\", \"input_ids2\")\n",
    "wordpiece_tokenized_text = wordpiece_tokenized_text.rename_column(\"attention_mask\", \"attention_mask2\")\n",
    "wordpiece_tokenized_text = wordpiece_tokenized_text.rename_column(\"labels\", \"labels2\")\n",
    "\n",
    "unigram_tokenized_text = ds.map(lambda examples: tokenize(examples, unigram_tokenizer), batched=True, remove_columns=['instruction',\"source\",\"score\",'output'])\n",
    "unigram_tokenized_text = unigram_tokenized_text.rename_column(\"input_ids\", \"input_ids3\")\n",
    "unigram_tokenized_text = unigram_tokenized_text.rename_column(\"attention_mask\", \"attention_mask3\")\n",
    "unigram_tokenized_text = unigram_tokenized_text.rename_column(\"labels\", \"labels3\")\n",
    "\n",
    "# Unify the tokenized datasets\n",
    "# Remove the 'token_type_ids' column from each dataset to avoid duplication\n",
    "# orig_tokenized_text = orig_tokenized_text.remove_columns(['token_type_ids'])\n",
    "wordpiece_tokenized_text = wordpiece_tokenized_text.remove_columns(['token_type_ids'])\n",
    "unigram_tokenized_text = unigram_tokenized_text.remove_columns(['token_type_ids'])\n",
    "\n",
    "# Unify the tokenized datasets\n",
    "unified_tokenized_text = datasets.concatenate_datasets([orig_tokenized_text, wordpiece_tokenized_text, unigram_tokenized_text], axis=1)\n",
    "\n",
    "\n",
    "del orig_tokenized_text\n",
    "del wordpiece_tokenized_text\n",
    "del unigram_tokenized_text\n",
    "# del unigram_tokenized_text\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb84fdb91c544c884222f457e709f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def concat_rows(row, max_length=1024):\n",
    "    sep = [200000]\n",
    "\n",
    "    combined_input_ids = row['input_ids1'] + sep + row['input_ids2'] + sep + row['input_ids3']\n",
    "    combined_attention_mask = row['attention_mask1'] + sep + row['attention_mask2'] + sep + row['attention_mask3']\n",
    "    combined_labels = row['labels1'] \n",
    "\n",
    "    # # Truncate if too long\n",
    "    # combined_input_ids = combined_input_ids[:max_length]\n",
    "    # combined_attention_mask = combined_attention_mask[:max_length]\n",
    "    # combined_labels = combined_labels[:max_length]\n",
    "\n",
    "    # # Pad if too short\n",
    "    # pad_id = 0  # or your tokenizer's pad_token_id\n",
    "    # pad_len = max_length - len(combined_input_ids)\n",
    "    # if pad_len > 0:\n",
    "    #     combined_input_ids += [pad_id] * pad_len\n",
    "    #     combined_attention_mask += [0] * pad_len\n",
    "    #     combined_labels += [pad_id] * pad_len\n",
    "\n",
    "    return {\n",
    "        'input_ids': combined_input_ids,\n",
    "        'attention_mask': combined_attention_mask,\n",
    "        'labels': combined_labels\n",
    "    }\n",
    "\n",
    "\n",
    "# unified_tokenized_text = unified_tokenized_text.map(concat_rows, remove_columns=['input_ids1', 'input_ids2', 'input_ids3', 'attention_mask1', 'attention_mask2', 'attention_mask3', 'labels1', 'labels2', 'labels3'], num_proc=6)\n",
    "# unified_tokenized_text = unified_tokenized_text.select(range(1000))  # Select a smaller subset for testing\n",
    "# unified_tokenized_text.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "unified_tokenized_text = unified_tokenized_text.map(\n",
    "    lambda row: concat_rows(row, max_length=1024),\n",
    "    remove_columns=['input_ids1', 'input_ids2', 'input_ids3', 'attention_mask1', 'attention_mask2', 'attention_mask3', 'labels1', 'labels2', 'labels3']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3074\n",
      "1024\n",
      "3074\n"
     ]
    }
   ],
   "source": [
    "print(len(unified_tokenized_text[0]['input_ids']))\n",
    "print(len(unified_tokenized_text[0]['labels']))\n",
    "print(len(unified_tokenized_text[0]['attention_mask']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "print(model.config.max_position_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yarab\\AppData\\Local\\Temp\\ipykernel_20272\\915514245.py:58: FutureWarning: `num_logits_to_keep` is deprecated and will be removed in version 4.50 for `LlamaForCausalLM.forward`. Use `logits_to_keep` instead.\n",
      "  return self.original_forward(\n",
      "C:\\Users\\yarab\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\integrations\\sdpa_attention.py:53: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1003' max='150000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  1003/150000 02:14 < 5:33:53, 7.44 it/s, Epoch 0.02/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>9.320600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>9.049300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 56\u001b[0m\n\u001b[0;32m     35\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m     36\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./trained_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     37\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,  \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m     optim\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madamw_torch\u001b[39m\u001b[38;5;124m\"\u001b[39m  \n\u001b[0;32m     45\u001b[0m )\n\u001b[0;32m     47\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     48\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     49\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m     optimizers\u001b[38;5;241m=\u001b[39m(optimizer, \u001b[38;5;28;01mNone\u001b[39;00m)  \n\u001b[0;32m     53\u001b[0m )\n\u001b[1;32m---> 56\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\trainer.py:2553\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m   2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2554\u001b[0m ):\n\u001b[0;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2557\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "model.train()\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "        {\"params\": model.model.extra_embedding_1.parameters(), \"lr\": 2e-5},\n",
    "        {\"params\": model.model.extra_embedding_2.parameters(), \"lr\": 2e-5}\n",
    "])\n",
    "\n",
    "def multi_tokenizer_data_collator(features):\n",
    "    \"\"\"\n",
    "    Custom data collator for handling input from multiple tokenizers\n",
    "    with separator tokens (200000) preserved.\n",
    "    \"\"\"\n",
    "    # Input IDs with batch dimension\n",
    "    input_ids = torch.stack([torch.tensor(f[\"input_ids\"]) for f in features])\n",
    "    attention_mask = torch.stack([torch.tensor(f[\"attention_mask\"]) for f in features])\n",
    "    labels = torch.stack([torch.tensor(f[\"labels\"]) for f in features])\n",
    "\n",
    "    # print(input_ids.shape, labels.shape, attention_mask.shape)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,  \n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "# Create the data collator\n",
    "data_collator = multi_tokenizer_data_collator\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./trained_embeddings\",\n",
    "    per_device_train_batch_size=2,  \n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    remove_unused_columns=False,  \n",
    "    fp16=True,  \n",
    "    optim=\"adamw_torch\"  \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=unified_tokenized_text,\n",
    "    data_collator=data_collator,\n",
    "    optimizers=(optimizer, None)  \n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(\"trained_embeddings\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
