{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":311615,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":264290,"modelId":285385}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin(token = \"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\") # Replace YOUR_TOKEN with your Hugging Face token\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T18:15:47.643081Z","iopub.execute_input":"2025-03-31T18:15:47.643341Z","iopub.status.idle":"2025-03-31T18:15:48.559396Z","shell.execute_reply.started":"2025-03-31T18:15:47.643312Z","shell.execute_reply":"2025-03-31T18:15:48.558275Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## LLM model initialization","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM\n\noriginal_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\nmodel = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T18:15:48.560363Z","iopub.execute_input":"2025-03-31T18:15:48.560658Z","iopub.status.idle":"2025-03-31T18:16:37.553065Z","shell.execute_reply.started":"2025-03-31T18:15:48.560610Z","shell.execute_reply":"2025-03-31T18:16:37.551938Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a50471353b441628d734fba93122fdf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b9282bf38674181a9e8a7f6157bf7af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e81c84b4b3c4e8386d8db523efcba6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e801feb1e414a95bf7bb59283a86d4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3001446a25174a36929e1492309d03d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"831f20a48c8f41ab9142a3453a46233d"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T18:16:37.554035Z","iopub.execute_input":"2025-03-31T18:16:37.554732Z","iopub.status.idle":"2025-03-31T18:16:37.561989Z","shell.execute_reply.started":"2025-03-31T18:16:37.554697Z","shell.execute_reply":"2025-03-31T18:16:37.560996Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 2048)\n    (layers): ModuleList(\n      (0-15): 16 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n)"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"## FVT method function","metadata":{}},{"cell_type":"code","source":"# from transformers import AutoTokenizer\n# import torch\n# import torch.nn as nn\n# import torch.nn.functional as F\n# from typing import Dict, List\n# import numpy as np\n# from tokenizers import Tokenizer\n\n# # Функция для FVT переноса эмбеддингов\n# def transfer_embeddings_with_fvt(\n#     original_tokenizer: AutoTokenizer,\n#     new_tokenizer: Tokenizer,\n#     original_embeddings: nn.Embedding,\n#     new_vocab_size: int,\n#     embedding_dim: int\n# ) -> nn.Embedding:\n#     # Создаем новый слой эмбеддингов\n#     new_embeddings = nn.Embedding(new_vocab_size, embedding_dim)\n    \n#     # Получаем словари токенов\n#     original_vocab = original_tokenizer.get_vocab()\n#     new_vocab = new_tokenizer.get_vocab()\n    \n#     # Для каждого токена в новом токенизаторе\n#     for token, idx in new_vocab.items():\n#         if token in original_vocab:\n#             # Случай 1: токен есть в обоих токенизаторах - копируем эмбеддинг\n#             original_idx = original_vocab[token]\n#             new_embeddings.weight.data[idx] = original_embeddings.weight.data[original_idx]\n#         else:\n#             # Разбиваем токен на части (для BPE-like токенизаторов)\n#             sub_tokens = token.split(' ') if ' ' in token else [token]\n            \n#             # Ищем соответствующие эмбеддинги для частей\n#             sub_embeddings = []\n#             for sub_token in sub_tokens:\n#                 if sub_token in original_vocab:\n#                     # Часть есть в оригинальном словаре\n#                     original_idx = original_vocab[sub_token]\n#                     sub_embeddings.append(original_embeddings.weight.data[original_idx])\n#                 else:\n#                     # Часть новая - ищем ближайший эмбеддинг\n#                     if len(original_vocab) > 0:\n#                         # Вычисляем косинусное расстояние\n#                         token_vec = torch.randn(embedding_dim).to(original_embeddings.weight.device)\n#                         sims = F.cosine_similarity(token_vec.unsqueeze(0), original_embeddings.weight, dim=1)\n#                         closest_idx = torch.argmax(sims)\n#                         sub_embeddings.append(original_embeddings.weight.data[closest_idx])\n            \n#             if len(sub_embeddings) > 0:\n#                 # Случай 2: усредняем эмбеддинги частей\n#                 new_embeddings.weight.data[idx] = torch.mean(torch.stack(sub_embeddings), dim=0)\n#             else:\n#                 # Случай 4: инициализируем случайно (но близко к среднему оригинальных эмбеддингов)\n#                 mean_embedding = torch.mean(original_embeddings.weight.data, dim=0)\n#                 noise = torch.randn(embedding_dim).to(original_embeddings.weight.device) * 0.1\n#                 new_embeddings.weight.data[idx] = mean_embedding + noise\n    \n#     return new_embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T17:52:52.786022Z","iopub.execute_input":"2025-03-31T17:52:52.786313Z","iopub.status.idle":"2025-03-31T17:52:52.794295Z","shell.execute_reply.started":"2025-03-31T17:52:52.786287Z","shell.execute_reply":"2025-03-31T17:52:52.793424Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from tqdm.auto import tqdm\nfrom collections import defaultdict\nimport math\nfrom transformers import AutoTokenizer\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import Dict, List\nimport numpy as np\nfrom tokenizers import Tokenizer\n\ndef transfer_embeddings_with_fvt(\n    original_tokenizer: AutoTokenizer,\n    new_tokenizer: Tokenizer,\n    original_embeddings: nn.Embedding,\n    new_vocab_size: int,\n    embedding_dim: int,\n    verbose: bool = True\n) -> nn.Embedding:\n    # Create new embedding layer\n    new_embeddings = nn.Embedding(new_vocab_size, embedding_dim)\n    device = original_embeddings.weight.device\n    new_embeddings = new_embeddings.to(device)\n    \n    # Get vocabularies\n    original_vocab = original_tokenizer.get_vocab()\n    new_vocab = new_tokenizer.get_vocab()\n    \n    # Statistics counters\n    stats = defaultdict(int)\n    stats['total_tokens'] = len(new_vocab)\n    \n    # Progress bar setup\n    pbar = tqdm(\n        new_vocab.items(), \n        desc=\"Transferring embeddings\", \n        disable=not verbose,\n        bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}] {postfix}\"\n    )\n    \n    # Pre-compute original embeddings for similarity search\n    original_emb_matrix = original_embeddings.weight.data\n    original_emb_norm = original_emb_matrix / original_emb_matrix.norm(dim=1, keepdim=True)\n    \n    for token, idx in pbar:\n        if token in original_vocab:\n            # Случай 1: токен есть в обоих токенизаторах - копируем эмбеддинг\n            original_idx = original_vocab[token]\n            new_embeddings.weight.data[idx] = original_embeddings.weight.data[original_idx]\n            stats['direct_transfer'] += 1\n            pbar.set_postfix({\n                'Direct': stats['direct_transfer'],\n                'Split': stats['split_transfer'],\n                'Closest': stats['closest_match'],\n                'Random': stats['random_init']\n            })\n        else:\n            # Разбиваем токен на части (для BPE-like токенизаторов)\n            sub_tokens = token.split(' ') if ' ' in token else [token]\n            sub_embeddings = []\n            \n            for sub_token in sub_tokens:\n                if sub_token in original_vocab:\n                    original_idx = original_vocab[sub_token]\n                    sub_embeddings.append(original_embeddings.weight.data[original_idx])\n                else:\n                    # Ищем соответствующие эмбеддинги для частей\n                    if len(original_vocab) > 0:\n                        sub_token_vec = torch.randn(embedding_dim, device=device)\n                        sub_token_vec = sub_token_vec / sub_token_vec.norm()\n                        \n                        # Вычисляем косинусное расстояние\n                        sims = torch.mv(original_emb_norm, sub_token_vec)\n                        closest_idx = torch.argmax(sims)\n                        sub_embeddings.append(original_embeddings.weight.data[closest_idx])\n                        stats['closest_match'] += 1\n            \n            if len(sub_embeddings) > 0:\n                # Случай 2: усредняем эмбеддинги частей\n                new_embeddings.weight.data[idx] = torch.mean(torch.stack(sub_embeddings), dim=0)\n                stats['split_transfer'] += 1\n            else:\n                # Случай 4: инициализируем случайно (но близко к среднему оригинальных эмбеддингов)\n                mean_embedding = torch.mean(original_embeddings.weight.data, dim=0)\n                noise = torch.randn(embedding_dim, device=device) * 0.1\n                new_embeddings.weight.data[idx] = mean_embedding + noise\n                stats['random_init'] += 1\n            \n            pbar.set_postfix({\n                'Direct': stats['direct_transfer'],\n                'Split': stats['split_transfer'],\n                'Closest': stats['closest_match'],\n                'Random': stats['random_init']\n            })\n    \n    # Print final statistics\n    if verbose:\n        print(\"\\nFVT Transfer Statistics:\")\n        print(f\"Total tokens processed: {stats['total_tokens']}\")\n        print(f\"Direct transfers: {stats['direct_transfer']} ({stats['direct_transfer']/stats['total_tokens']:.1%})\")\n        print(f\"Split tokens averaged: {stats['split_transfer']} ({stats['split_transfer']/stats['total_tokens']:.1%})\")\n        print(f\"Closest matches used: {stats['closest_match']}\")\n        print(f\"Random initializations: {stats['random_init']} ({stats['random_init']/stats['total_tokens']:.1%})\")\n    \n    return new_embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T18:16:37.563099Z","iopub.execute_input":"2025-03-31T18:16:37.563373Z","iopub.status.idle":"2025-03-31T18:16:45.116677Z","shell.execute_reply.started":"2025-03-31T18:16:37.563350Z","shell.execute_reply":"2025-03-31T18:16:45.115606Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from tokenizers import Tokenizer\n\nunigram_tokenizer = Tokenizer.from_file(\"/kaggle/input/tokenizers/pytorch/default/1/unigram_tokenizer-5.json\")\nwordpiece_tokenizer = Tokenizer.from_file(\"/kaggle/input/tokenizers/pytorch/default/1/wordpiece_tokenizer (3).json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T18:16:45.117800Z","iopub.execute_input":"2025-03-31T18:16:45.118163Z","iopub.status.idle":"2025-03-31T18:16:45.861998Z","shell.execute_reply.started":"2025-03-31T18:16:45.118125Z","shell.execute_reply":"2025-03-31T18:16:45.861004Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nfrom transformers import pipeline\nimport torch.nn as nn\n\noriginal_vocab_size, embedding_dim = model.model.embed_tokens.weight.shape\n\n# Инициализируем с использованием FVT\n# print(\"Transferring WordPiece embeddings:\")\n# extra_embedding_1 = transfer_embeddings_with_fvt(\n#     original_tokenizer,\n#     wordpiece_tokenizer,\n#     model.model.embed_tokens,\n#     original_vocab_size,\n#     embedding_dim\n# )\n\nprint(\"\\nTransferring Unigram embeddings:\")\nextra_embedding_2 = transfer_embeddings_with_fvt(\n    original_tokenizer,\n    unigram_tokenizer,\n    model.model.embed_tokens,\n    original_vocab_size,\n    embedding_dim\n)\n\n# nn.init.xavier_uniform_(extra_embedding_1.weight)\nnn.init.xavier_uniform_(extra_embedding_2.weight)\n\n# extra_embedding_1.weight.requires_grad = True\nextra_embedding_2.weight.requires_grad = True\n\n# Add the new embeddings as attributes\nmodel.model.extra_embedding_1 = extra_embedding_1\nmodel.model.extra_embedding_2 = extra_embedding_2\n\n# Copy weights\n# model.model.extra_embedding_1.weight.data.copy_(model.model.embed_tokens.weight.data)\nmodel.model.extra_embedding_2.weight.data.copy_(model.model.embed_tokens.weight.data)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T18:22:37.449881Z","iopub.execute_input":"2025-03-31T18:22:37.450301Z"}},"outputs":[{"name":"stdout","text":"\nTransferring Unigram embeddings:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Transferring embeddings:   0%|          | 0/128256 [00:00<?, ?it/s] ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a231f269e6a4766aa8474d548aea475"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"# Сохранение эмбеддингов в файлы\n# torch.save(extra_embedding_1.state_dict(), \"extra_embedding_1.pth\")\ntorch.save(extra_embedding_2.state_dict(), \"extra_embedding_2.pth\")\n\nprint(\"Extra embeddings saved successfully.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Загрузка эмбеддингов\nextra_embedding_1.load_state_dict(torch.load(\"extra_embedding_1.pth\"))\nextra_embedding_2.load_state_dict(torch.load(\"extra_embedding_2.pth\"))\n\nprint(\"Extra embeddings loaded successfully.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model","metadata":{},"outputs":[{"data":{"text/plain":["LlamaForCausalLM(\n","  (model): LlamaModel(\n","    (embed_tokens): Embedding(128256, 2048)\n","    (layers): ModuleList(\n","      (0-15): 16 x LlamaDecoderLayer(\n","        (self_attn): LlamaAttention(\n","          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n","          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n","          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n","          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n","        )\n","        (mlp): LlamaMLP(\n","          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n","          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n","          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n","        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n","      )\n","    )\n","    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n","    (rotary_emb): LlamaRotaryEmbedding()\n","    (extra_embedding_1): Embedding(128256, 2048)\n","    (extra_embedding_2): Embedding(128256, 2048)\n","  )\n","  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",")"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"execution_count":20},{"cell_type":"code","source":"from typing import List, Optional, Union\nfrom cachetools import Cache\nimport types\n\ndef modified_forward(\n    self,\n    input_ids: torch.LongTensor = None,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n    inputs_embeds: Optional[torch.FloatTensor] = None,\n    labels: Optional[torch.LongTensor] = None,\n    use_cache: Optional[bool] = None,\n    output_attentions: Optional[bool] = None,\n    output_hidden_states: Optional[bool] = None,\n    return_dict: Optional[bool] = None,\n    cache_position: Optional[torch.LongTensor] = None,\n    num_logits_to_keep: int = 0,\n    **kwargs\n):\n    if input_ids is None and inputs_embeds is None:\n        raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n    batch_size = input_ids.shape[0]\n    combined_embeds = []\n\n    for batch_idx in range(batch_size):\n        str_input_ids = \" \".join([str(i) for i in input_ids[batch_idx].tolist()])\n        input_ids_parts = str_input_ids.split(\" 200000 \")\n        \n        bpe = torch.tensor([list(map(int, input_ids_parts[0].split(\" \")))], \n                           device=input_ids.device)\n        unigram = torch.tensor([list(map(int, input_ids_parts[1].split(\" \")))], \n                               device=input_ids.device)\n        sentencepiece = torch.tensor([list(map(int, input_ids_parts[2].split(\" \")))], \n                                     device=input_ids.device)\n        \n        bpe_embedding = self.model.embed_tokens(bpe)\n        unigram_embedding = self.model.extra_embedding_1(unigram)\n        sentencepiece_embedding = self.model.extra_embedding_2(sentencepiece)\n        \n        print(f\"Shapes: BPE {bpe_embedding.shape}, Unigram {unigram_embedding.shape}, SentencePiece {sentencepiece_embedding.shape}\")\n        \n        min_length = min(bpe_embedding.shape[1], unigram_embedding.shape[1], sentencepiece_embedding.shape[1])\n        bpe_embedding = bpe_embedding[:, :min_length, :]\n        unigram_embedding = unigram_embedding[:, :min_length, :]\n        sentencepiece_embedding = sentencepiece_embedding[:, :min_length, :]\n        \n        batch_embeds = bpe_embedding + unigram_embedding + sentencepiece_embedding\n        combined_embeds.append(batch_embeds)\n\n    inputs_embeds = torch.cat(combined_embeds, dim=0)\n    print(f\"Final inputs_embeds shape: {inputs_embeds.shape}\")\n    \n    if attention_mask is not None:\n        attention_mask = attention_mask[:, :inputs_embeds.shape[1]]\n    \n    return self.original_forward(\n        input_ids=None,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        past_key_values=past_key_values,\n        inputs_embeds=inputs_embeds,\n        use_cache=use_cache,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n        cache_position=cache_position,\n        labels=labels,\n        num_logits_to_keep=num_logits_to_keep,\n        **kwargs\n    )\n    \nmodel.original_forward = model.forward\nmodel.forward = types.MethodType(modified_forward, model)\n","metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Freeze everething, except the new embeddings\nfor param in model.parameters():\n    param.requires_grad = False\n\nfor param in model.model.extra_embedding_1.parameters():\n    param.requires_grad = True\n\nfor param in model.model.extra_embedding_2.parameters():\n    param.requires_grad = True\n","metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":"#check if the embeddings are trainable\nfor name, param in model.named_parameters():\n    print(name, param.requires_grad)\n","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["model.embed_tokens.weight False\n","model.layers.0.self_attn.q_proj.weight False\n","model.layers.0.self_attn.k_proj.weight False\n","model.layers.0.self_attn.v_proj.weight False\n","model.layers.0.self_attn.o_proj.weight False\n","model.layers.0.mlp.gate_proj.weight False\n","model.layers.0.mlp.up_proj.weight False\n","model.layers.0.mlp.down_proj.weight False\n","model.layers.0.input_layernorm.weight False\n","model.layers.0.post_attention_layernorm.weight False\n","model.layers.1.self_attn.q_proj.weight False\n","model.layers.1.self_attn.k_proj.weight False\n","model.layers.1.self_attn.v_proj.weight False\n","model.layers.1.self_attn.o_proj.weight False\n","model.layers.1.mlp.gate_proj.weight False\n","model.layers.1.mlp.up_proj.weight False\n","model.layers.1.mlp.down_proj.weight False\n","model.layers.1.input_layernorm.weight False\n","model.layers.1.post_attention_layernorm.weight False\n","model.layers.2.self_attn.q_proj.weight False\n","model.layers.2.self_attn.k_proj.weight False\n","model.layers.2.self_attn.v_proj.weight False\n","model.layers.2.self_attn.o_proj.weight False\n","model.layers.2.mlp.gate_proj.weight False\n","model.layers.2.mlp.up_proj.weight False\n","model.layers.2.mlp.down_proj.weight False\n","model.layers.2.input_layernorm.weight False\n","model.layers.2.post_attention_layernorm.weight False\n","model.layers.3.self_attn.q_proj.weight False\n","model.layers.3.self_attn.k_proj.weight False\n","model.layers.3.self_attn.v_proj.weight False\n","model.layers.3.self_attn.o_proj.weight False\n","model.layers.3.mlp.gate_proj.weight False\n","model.layers.3.mlp.up_proj.weight False\n","model.layers.3.mlp.down_proj.weight False\n","model.layers.3.input_layernorm.weight False\n","model.layers.3.post_attention_layernorm.weight False\n","model.layers.4.self_attn.q_proj.weight False\n","model.layers.4.self_attn.k_proj.weight False\n","model.layers.4.self_attn.v_proj.weight False\n","model.layers.4.self_attn.o_proj.weight False\n","model.layers.4.mlp.gate_proj.weight False\n","model.layers.4.mlp.up_proj.weight False\n","model.layers.4.mlp.down_proj.weight False\n","model.layers.4.input_layernorm.weight False\n","model.layers.4.post_attention_layernorm.weight False\n","model.layers.5.self_attn.q_proj.weight False\n","model.layers.5.self_attn.k_proj.weight False\n","model.layers.5.self_attn.v_proj.weight False\n","model.layers.5.self_attn.o_proj.weight False\n","model.layers.5.mlp.gate_proj.weight False\n","model.layers.5.mlp.up_proj.weight False\n","model.layers.5.mlp.down_proj.weight False\n","model.layers.5.input_layernorm.weight False\n","model.layers.5.post_attention_layernorm.weight False\n","model.layers.6.self_attn.q_proj.weight False\n","model.layers.6.self_attn.k_proj.weight False\n","model.layers.6.self_attn.v_proj.weight False\n","model.layers.6.self_attn.o_proj.weight False\n","model.layers.6.mlp.gate_proj.weight False\n","model.layers.6.mlp.up_proj.weight False\n","model.layers.6.mlp.down_proj.weight False\n","model.layers.6.input_layernorm.weight False\n","model.layers.6.post_attention_layernorm.weight False\n","model.layers.7.self_attn.q_proj.weight False\n","model.layers.7.self_attn.k_proj.weight False\n","model.layers.7.self_attn.v_proj.weight False\n","model.layers.7.self_attn.o_proj.weight False\n","model.layers.7.mlp.gate_proj.weight False\n","model.layers.7.mlp.up_proj.weight False\n","model.layers.7.mlp.down_proj.weight False\n","model.layers.7.input_layernorm.weight False\n","model.layers.7.post_attention_layernorm.weight False\n","model.layers.8.self_attn.q_proj.weight False\n","model.layers.8.self_attn.k_proj.weight False\n","model.layers.8.self_attn.v_proj.weight False\n","model.layers.8.self_attn.o_proj.weight False\n","model.layers.8.mlp.gate_proj.weight False\n","model.layers.8.mlp.up_proj.weight False\n","model.layers.8.mlp.down_proj.weight False\n","model.layers.8.input_layernorm.weight False\n","model.layers.8.post_attention_layernorm.weight False\n","model.layers.9.self_attn.q_proj.weight False\n","model.layers.9.self_attn.k_proj.weight False\n","model.layers.9.self_attn.v_proj.weight False\n","model.layers.9.self_attn.o_proj.weight False\n","model.layers.9.mlp.gate_proj.weight False\n","model.layers.9.mlp.up_proj.weight False\n","model.layers.9.mlp.down_proj.weight False\n","model.layers.9.input_layernorm.weight False\n","model.layers.9.post_attention_layernorm.weight False\n","model.layers.10.self_attn.q_proj.weight False\n","model.layers.10.self_attn.k_proj.weight False\n","model.layers.10.self_attn.v_proj.weight False\n","model.layers.10.self_attn.o_proj.weight False\n","model.layers.10.mlp.gate_proj.weight False\n","model.layers.10.mlp.up_proj.weight False\n","model.layers.10.mlp.down_proj.weight False\n","model.layers.10.input_layernorm.weight False\n","model.layers.10.post_attention_layernorm.weight False\n","model.layers.11.self_attn.q_proj.weight False\n","model.layers.11.self_attn.k_proj.weight False\n","model.layers.11.self_attn.v_proj.weight False\n","model.layers.11.self_attn.o_proj.weight False\n","model.layers.11.mlp.gate_proj.weight False\n","model.layers.11.mlp.up_proj.weight False\n","model.layers.11.mlp.down_proj.weight False\n","model.layers.11.input_layernorm.weight False\n","model.layers.11.post_attention_layernorm.weight False\n","model.layers.12.self_attn.q_proj.weight False\n","model.layers.12.self_attn.k_proj.weight False\n","model.layers.12.self_attn.v_proj.weight False\n","model.layers.12.self_attn.o_proj.weight False\n","model.layers.12.mlp.gate_proj.weight False\n","model.layers.12.mlp.up_proj.weight False\n","model.layers.12.mlp.down_proj.weight False\n","model.layers.12.input_layernorm.weight False\n","model.layers.12.post_attention_layernorm.weight False\n","model.layers.13.self_attn.q_proj.weight False\n","model.layers.13.self_attn.k_proj.weight False\n","model.layers.13.self_attn.v_proj.weight False\n","model.layers.13.self_attn.o_proj.weight False\n","model.layers.13.mlp.gate_proj.weight False\n","model.layers.13.mlp.up_proj.weight False\n","model.layers.13.mlp.down_proj.weight False\n","model.layers.13.input_layernorm.weight False\n","model.layers.13.post_attention_layernorm.weight False\n","model.layers.14.self_attn.q_proj.weight False\n","model.layers.14.self_attn.k_proj.weight False\n","model.layers.14.self_attn.v_proj.weight False\n","model.layers.14.self_attn.o_proj.weight False\n","model.layers.14.mlp.gate_proj.weight False\n","model.layers.14.mlp.up_proj.weight False\n","model.layers.14.mlp.down_proj.weight False\n","model.layers.14.input_layernorm.weight False\n","model.layers.14.post_attention_layernorm.weight False\n","model.layers.15.self_attn.q_proj.weight False\n","model.layers.15.self_attn.k_proj.weight False\n","model.layers.15.self_attn.v_proj.weight False\n","model.layers.15.self_attn.o_proj.weight False\n","model.layers.15.mlp.gate_proj.weight False\n","model.layers.15.mlp.up_proj.weight False\n","model.layers.15.mlp.down_proj.weight False\n","model.layers.15.input_layernorm.weight False\n","model.layers.15.post_attention_layernorm.weight False\n","model.norm.weight False\n","model.extra_embedding_1.weight True\n","model.extra_embedding_2.weight True\n"]}],"execution_count":23},{"cell_type":"code","source":"from datasets import load_dataset\n\nds = load_dataset(\"mlabonne/FineTome-Alpaca-100k\", split=\"train\")\n","metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":"print(ds[0])\n","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'source': 'infini-instruct-top-500k', 'score': 5.212620735168457, 'instruction': 'Explain what boolean operators are, what they do, and provide examples of how they can be used in programming. Additionally, describe the concept of operator precedence and provide examples of how it affects the evaluation of boolean expressions. Discuss the difference between short-circuit evaluation and normal evaluation in boolean expressions and demonstrate their usage in code. \\n\\nFurthermore, add the requirement that the code must be written in a language that does not support short-circuit evaluation natively, forcing the test taker to implement their own logic for short-circuit evaluation.\\n\\nFinally, delve into the concept of truthiness and falsiness in programming languages, explaining how it affects the evaluation of boolean expressions. Add the constraint that the test taker must write code that handles cases where truthiness and falsiness are implemented differently across different programming languages.', 'output': 'Boolean operators are logical operators used in programming to manipulate boolean values. They operate on one or more boolean operands and return a boolean result. The three main boolean operators are \"AND\" (&&), \"OR\" (||), and \"NOT\" (!).\\n\\nThe \"AND\" operator returns true if both of its operands are true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) and (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"OR\" operator returns true if at least one of its operands is true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) or (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"NOT\" operator negates the boolean value of its operand. It returns true if the operand is false, and false if the operand is true. For example:\\n\\n```python\\nx = 5\\nresult = not (x > 10)  # This expression evaluates to True\\n```\\n\\nOperator precedence refers to the order in which operators are evaluated in an expression. It ensures that expressions are evaluated correctly. In most programming languages, logical AND has higher precedence than logical OR. For example:\\n\\n```python\\nresult = True or False and False  # This expression is evaluated as (True or (False and False)), which is True\\n```\\n\\nShort-circuit evaluation is a behavior where the second operand of a logical operator is not evaluated if the result can be determined based on the value of the first operand. In short-circuit evaluation, if the first operand of an \"AND\" operator is false, the second operand is not evaluated because the result will always be false. Similarly, if the first operand of an \"OR\" operator is true, the second operand is not evaluated because the result will always be true.\\n\\nIn programming languages that support short-circuit evaluation natively, you can use it to improve performance or avoid errors. For example:\\n\\n```python\\nif x != 0 and (y / x) > 10:\\n    # Perform some operation\\n```\\n\\nIn languages without native short-circuit evaluation, you can implement your own logic to achieve the same behavior. Here\\'s an example in pseudocode:\\n\\n```\\nif x != 0 {\\n    if (y / x) > 10 {\\n        // Perform some operation\\n    }\\n}\\n```\\n\\nTruthiness and falsiness refer to how non-boolean values are evaluated in boolean contexts. In many programming languages, non-zero numbers and non-empty strings are considered truthy, while zero, empty strings, and null/None values are considered falsy.\\n\\nWhen evaluating boolean expressions, truthiness and falsiness come into play. For example:\\n\\n```python\\nx = 5\\nresult = x  # The value of x is truthy, so result is also truthy\\n```\\n\\nTo handle cases where truthiness and falsiness are implemented differently across programming languages, you can explicitly check the desired condition. For example:\\n\\n```python\\nx = 5\\nresult = bool(x)  # Explicitly converting x to a boolean value\\n```\\n\\nThis ensures that the result is always a boolean value, regardless of the language\\'s truthiness and falsiness rules.'}\n"]}],"execution_count":25},{"cell_type":"code","source":"def tokenize(examples, tokenizer):\n    texts = [f\"### Instruction: {instruction}\\n### Response: {output}\" \n             for instruction, output in zip(examples['instruction'], examples['output'])]\n    \n    # Handle different tokenizer types\n    if hasattr(tokenizer, 'encode') and not callable(tokenizer):\n        # For tokenizers.Tokenizer objects (unigram and wordpiece)\n        results = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n        \n        for text in texts:\n            encoded = tokenizer.encode(text)\n            ids = encoded.ids[:1024]  # Truncate to max_length\n            \n            # Padding\n            attention = [1] * len(ids)\n            padding_length = 1024 - len(ids)\n            if padding_length > 0:\n                ids = ids + [0] * padding_length  # Assuming 0 is pad token ID\n                attention = attention + [0] * padding_length\n                \n            results[\"input_ids\"].append(ids)\n            results[\"attention_mask\"].append(attention)\n            results[\"labels\"].append(ids.copy())\n            \n        return results\n    else:\n        # For transformers tokenizers (original_tokenizer)\n        tokenized = tokenizer(\n            texts,\n            truncation=True,\n            max_length=1024,\n            padding=\"max_length\",\n            return_tensors=None\n        )\n        \n        # Add labels for causal language modeling\n        tokenized[\"labels\"] = [ids.copy() for ids in tokenized[\"input_ids\"]]\n        return tokenized","metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# from tokenizers import Tokenizer\n\n# unigram_tokenizer = Tokenizer.from_file(\"unigram_tokenizer-4.json\")\n# wordpiece_tokenizer = Tokenizer.from_file(\"wordpiece_tokenizer-2.json\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print all special tokens in the original tokenizer\nprint(original_tokenizer.special_tokens_map)\nprint(original_tokenizer.get_added_vocab())","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'bos_token': '<|begin_of_text|>', 'eos_token': '<|eot_id|>'}\n","{'<|begin_of_text|>': 128000, '<|end_of_text|>': 128001, '<|reserved_special_token_0|>': 128002, '<|reserved_special_token_1|>': 128003, '<|finetune_right_pad_id|>': 128004, '<|reserved_special_token_2|>': 128005, '<|start_header_id|>': 128006, '<|end_header_id|>': 128007, '<|eom_id|>': 128008, '<|eot_id|>': 128009, '<|python_tag|>': 128010, '<|reserved_special_token_3|>': 128011, '<|reserved_special_token_4|>': 128012, '<|reserved_special_token_5|>': 128013, '<|reserved_special_token_6|>': 128014, '<|reserved_special_token_7|>': 128015, '<|reserved_special_token_8|>': 128016, '<|reserved_special_token_9|>': 128017, '<|reserved_special_token_10|>': 128018, '<|reserved_special_token_11|>': 128019, '<|reserved_special_token_12|>': 128020, '<|reserved_special_token_13|>': 128021, '<|reserved_special_token_14|>': 128022, '<|reserved_special_token_15|>': 128023, '<|reserved_special_token_16|>': 128024, '<|reserved_special_token_17|>': 128025, '<|reserved_special_token_18|>': 128026, '<|reserved_special_token_19|>': 128027, '<|reserved_special_token_20|>': 128028, '<|reserved_special_token_21|>': 128029, '<|reserved_special_token_22|>': 128030, '<|reserved_special_token_23|>': 128031, '<|reserved_special_token_24|>': 128032, '<|reserved_special_token_25|>': 128033, '<|reserved_special_token_26|>': 128034, '<|reserved_special_token_27|>': 128035, '<|reserved_special_token_28|>': 128036, '<|reserved_special_token_29|>': 128037, '<|reserved_special_token_30|>': 128038, '<|reserved_special_token_31|>': 128039, '<|reserved_special_token_32|>': 128040, '<|reserved_special_token_33|>': 128041, '<|reserved_special_token_34|>': 128042, '<|reserved_special_token_35|>': 128043, '<|reserved_special_token_36|>': 128044, '<|reserved_special_token_37|>': 128045, '<|reserved_special_token_38|>': 128046, '<|reserved_special_token_39|>': 128047, '<|reserved_special_token_40|>': 128048, '<|reserved_special_token_41|>': 128049, '<|reserved_special_token_42|>': 128050, '<|reserved_special_token_43|>': 128051, '<|reserved_special_token_44|>': 128052, '<|reserved_special_token_45|>': 128053, '<|reserved_special_token_46|>': 128054, '<|reserved_special_token_47|>': 128055, '<|reserved_special_token_48|>': 128056, '<|reserved_special_token_49|>': 128057, '<|reserved_special_token_50|>': 128058, '<|reserved_special_token_51|>': 128059, '<|reserved_special_token_52|>': 128060, '<|reserved_special_token_53|>': 128061, '<|reserved_special_token_54|>': 128062, '<|reserved_special_token_55|>': 128063, '<|reserved_special_token_56|>': 128064, '<|reserved_special_token_57|>': 128065, '<|reserved_special_token_58|>': 128066, '<|reserved_special_token_59|>': 128067, '<|reserved_special_token_60|>': 128068, '<|reserved_special_token_61|>': 128069, '<|reserved_special_token_62|>': 128070, '<|reserved_special_token_63|>': 128071, '<|reserved_special_token_64|>': 128072, '<|reserved_special_token_65|>': 128073, '<|reserved_special_token_66|>': 128074, '<|reserved_special_token_67|>': 128075, '<|reserved_special_token_68|>': 128076, '<|reserved_special_token_69|>': 128077, '<|reserved_special_token_70|>': 128078, '<|reserved_special_token_71|>': 128079, '<|reserved_special_token_72|>': 128080, '<|reserved_special_token_73|>': 128081, '<|reserved_special_token_74|>': 128082, '<|reserved_special_token_75|>': 128083, '<|reserved_special_token_76|>': 128084, '<|reserved_special_token_77|>': 128085, '<|reserved_special_token_78|>': 128086, '<|reserved_special_token_79|>': 128087, '<|reserved_special_token_80|>': 128088, '<|reserved_special_token_81|>': 128089, '<|reserved_special_token_82|>': 128090, '<|reserved_special_token_83|>': 128091, '<|reserved_special_token_84|>': 128092, '<|reserved_special_token_85|>': 128093, '<|reserved_special_token_86|>': 128094, '<|reserved_special_token_87|>': 128095, '<|reserved_special_token_88|>': 128096, '<|reserved_special_token_89|>': 128097, '<|reserved_special_token_90|>': 128098, '<|reserved_special_token_91|>': 128099, '<|reserved_special_token_92|>': 128100, '<|reserved_special_token_93|>': 128101, '<|reserved_special_token_94|>': 128102, '<|reserved_special_token_95|>': 128103, '<|reserved_special_token_96|>': 128104, '<|reserved_special_token_97|>': 128105, '<|reserved_special_token_98|>': 128106, '<|reserved_special_token_99|>': 128107, '<|reserved_special_token_100|>': 128108, '<|reserved_special_token_101|>': 128109, '<|reserved_special_token_102|>': 128110, '<|reserved_special_token_103|>': 128111, '<|reserved_special_token_104|>': 128112, '<|reserved_special_token_105|>': 128113, '<|reserved_special_token_106|>': 128114, '<|reserved_special_token_107|>': 128115, '<|reserved_special_token_108|>': 128116, '<|reserved_special_token_109|>': 128117, '<|reserved_special_token_110|>': 128118, '<|reserved_special_token_111|>': 128119, '<|reserved_special_token_112|>': 128120, '<|reserved_special_token_113|>': 128121, '<|reserved_special_token_114|>': 128122, '<|reserved_special_token_115|>': 128123, '<|reserved_special_token_116|>': 128124, '<|reserved_special_token_117|>': 128125, '<|reserved_special_token_118|>': 128126, '<|reserved_special_token_119|>': 128127, '<|reserved_special_token_120|>': 128128, '<|reserved_special_token_121|>': 128129, '<|reserved_special_token_122|>': 128130, '<|reserved_special_token_123|>': 128131, '<|reserved_special_token_124|>': 128132, '<|reserved_special_token_125|>': 128133, '<|reserved_special_token_126|>': 128134, '<|reserved_special_token_127|>': 128135, '<|reserved_special_token_128|>': 128136, '<|reserved_special_token_129|>': 128137, '<|reserved_special_token_130|>': 128138, '<|reserved_special_token_131|>': 128139, '<|reserved_special_token_132|>': 128140, '<|reserved_special_token_133|>': 128141, '<|reserved_special_token_134|>': 128142, '<|reserved_special_token_135|>': 128143, '<|reserved_special_token_136|>': 128144, '<|reserved_special_token_137|>': 128145, '<|reserved_special_token_138|>': 128146, '<|reserved_special_token_139|>': 128147, '<|reserved_special_token_140|>': 128148, '<|reserved_special_token_141|>': 128149, '<|reserved_special_token_142|>': 128150, '<|reserved_special_token_143|>': 128151, '<|reserved_special_token_144|>': 128152, '<|reserved_special_token_145|>': 128153, '<|reserved_special_token_146|>': 128154, '<|reserved_special_token_147|>': 128155, '<|reserved_special_token_148|>': 128156, '<|reserved_special_token_149|>': 128157, '<|reserved_special_token_150|>': 128158, '<|reserved_special_token_151|>': 128159, '<|reserved_special_token_152|>': 128160, '<|reserved_special_token_153|>': 128161, '<|reserved_special_token_154|>': 128162, '<|reserved_special_token_155|>': 128163, '<|reserved_special_token_156|>': 128164, '<|reserved_special_token_157|>': 128165, '<|reserved_special_token_158|>': 128166, '<|reserved_special_token_159|>': 128167, '<|reserved_special_token_160|>': 128168, '<|reserved_special_token_161|>': 128169, '<|reserved_special_token_162|>': 128170, '<|reserved_special_token_163|>': 128171, '<|reserved_special_token_164|>': 128172, '<|reserved_special_token_165|>': 128173, '<|reserved_special_token_166|>': 128174, '<|reserved_special_token_167|>': 128175, '<|reserved_special_token_168|>': 128176, '<|reserved_special_token_169|>': 128177, '<|reserved_special_token_170|>': 128178, '<|reserved_special_token_171|>': 128179, '<|reserved_special_token_172|>': 128180, '<|reserved_special_token_173|>': 128181, '<|reserved_special_token_174|>': 128182, '<|reserved_special_token_175|>': 128183, '<|reserved_special_token_176|>': 128184, '<|reserved_special_token_177|>': 128185, '<|reserved_special_token_178|>': 128186, '<|reserved_special_token_179|>': 128187, '<|reserved_special_token_180|>': 128188, '<|reserved_special_token_181|>': 128189, '<|reserved_special_token_182|>': 128190, '<|reserved_special_token_183|>': 128191, '<|reserved_special_token_184|>': 128192, '<|reserved_special_token_185|>': 128193, '<|reserved_special_token_186|>': 128194, '<|reserved_special_token_187|>': 128195, '<|reserved_special_token_188|>': 128196, '<|reserved_special_token_189|>': 128197, '<|reserved_special_token_190|>': 128198, '<|reserved_special_token_191|>': 128199, '<|reserved_special_token_192|>': 128200, '<|reserved_special_token_193|>': 128201, '<|reserved_special_token_194|>': 128202, '<|reserved_special_token_195|>': 128203, '<|reserved_special_token_196|>': 128204, '<|reserved_special_token_197|>': 128205, '<|reserved_special_token_198|>': 128206, '<|reserved_special_token_199|>': 128207, '<|reserved_special_token_200|>': 128208, '<|reserved_special_token_201|>': 128209, '<|reserved_special_token_202|>': 128210, '<|reserved_special_token_203|>': 128211, '<|reserved_special_token_204|>': 128212, '<|reserved_special_token_205|>': 128213, '<|reserved_special_token_206|>': 128214, '<|reserved_special_token_207|>': 128215, '<|reserved_special_token_208|>': 128216, '<|reserved_special_token_209|>': 128217, '<|reserved_special_token_210|>': 128218, '<|reserved_special_token_211|>': 128219, '<|reserved_special_token_212|>': 128220, '<|reserved_special_token_213|>': 128221, '<|reserved_special_token_214|>': 128222, '<|reserved_special_token_215|>': 128223, '<|reserved_special_token_216|>': 128224, '<|reserved_special_token_217|>': 128225, '<|reserved_special_token_218|>': 128226, '<|reserved_special_token_219|>': 128227, '<|reserved_special_token_220|>': 128228, '<|reserved_special_token_221|>': 128229, '<|reserved_special_token_222|>': 128230, '<|reserved_special_token_223|>': 128231, '<|reserved_special_token_224|>': 128232, '<|reserved_special_token_225|>': 128233, '<|reserved_special_token_226|>': 128234, '<|reserved_special_token_227|>': 128235, '<|reserved_special_token_228|>': 128236, '<|reserved_special_token_229|>': 128237, '<|reserved_special_token_230|>': 128238, '<|reserved_special_token_231|>': 128239, '<|reserved_special_token_232|>': 128240, '<|reserved_special_token_233|>': 128241, '<|reserved_special_token_234|>': 128242, '<|reserved_special_token_235|>': 128243, '<|reserved_special_token_236|>': 128244, '<|reserved_special_token_237|>': 128245, '<|reserved_special_token_238|>': 128246, '<|reserved_special_token_239|>': 128247, '<|reserved_special_token_240|>': 128248, '<|reserved_special_token_241|>': 128249, '<|reserved_special_token_242|>': 128250, '<|reserved_special_token_243|>': 128251, '<|reserved_special_token_244|>': 128252, '<|reserved_special_token_245|>': 128253, '<|reserved_special_token_246|>': 128254, '<|reserved_special_token_247|>': 128255}\n"]}],"execution_count":28},{"cell_type":"code","source":"original_tokenizer.pad_token = original_tokenizer.eos_token\n\norig_tokenized_text = ds.map(lambda examples: tokenize(examples, original_tokenizer), batched=True, remove_columns=['instruction',\"source\",\"score\",'output'])\norig_tokenized_text.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\nwordpiece_tokenized_text = ds.map(lambda examples: tokenize(examples, wordpiece_tokenizer), batched=True, remove_columns=['instruction',\"source\",\"score\",'output'])\nwordpiece_tokenized_text.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\nunigram_tokenized_text = ds.map(lambda examples: tokenize(examples, unigram_tokenizer), batched=True, remove_columns=['instruction',\"source\",\"score\",'output'])\nunigram_tokenized_text.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n","metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"40fcdb65ebea49d98dc6b5cd146e2920","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"execution_count":31},{"cell_type":"code","source":"tokenized_text = []\ntokenized_text = orig_tokenized_text + [200000] + wordpiece_tokenized_text + [200000] + unigram_tokenized_text\ntokenized_text.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])","metadata":{},"outputs":[{"ename":"TypeError","evalue":"unsupported operand type(s) for +: 'Tensor' and 'list'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[35], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m tokenized_text \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 2\u001b[0m tokenized_text \u001b[38;5;241m=\u001b[39m \u001b[43morig_tokenized_text\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m200000\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m wordpiece_tokenized_text[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m200000\u001b[39m] \u001b[38;5;241m+\u001b[39m unigram_tokenized_text[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      3\u001b[0m tokenized_text\u001b[38;5;241m.\u001b[39mset_format(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n","\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'Tensor' and 'list'"]}],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n\nmodel.train()\n\noptimizer = torch.optim.AdamW([\n        {\"params\": model.model.embed_tokens.new_embeddings_1.parameters(), \"lr\": 2e-5, \"weight_decay\": 0.01},\n        {\"params\": model.model.embed_tokens.new_embeddings_2.parameters(), \"lr\": 2e-5, \"weight_decay\": 0.01}\n])\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False  \n)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./trained_embeddings\",\n    per_device_train_batch_size=4,  \n    num_train_epochs=3,\n    logging_dir=\"./logs\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    remove_unused_columns=False,  \n    fp16=True,  \n    optim=\"adamw_torch\"  \n)\n\n# Define Trainer with the data collator\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_ds,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    optimizers=(optimizer, None)  \n)\n\n\ntrainer.train()\n","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\yarab\\AppData\\Local\\Temp\\ipykernel_9064\\3236532317.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2' max='75000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [    2/75000 : < :, Epoch 0.00/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 1.96 GiB. GPU ","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[1;32mIn[15], line 42\u001b[0m\n\u001b[0;32m     32\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     33\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     34\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     optimizers\u001b[38;5;241m=\u001b[39m(optimizer, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# Pass the optimizer\u001b[39;00m\n\u001b[0;32m     39\u001b[0m )\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\trainer.py:2548\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2541\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2542\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2544\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2546\u001b[0m )\n\u001b[0;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2554\u001b[0m ):\n\u001b[0;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\trainer.py:3698\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3695\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3697\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3698\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3700\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3702\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3703\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3704\u001b[0m ):\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\trainer.py:3759\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3757\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[0;32m   3758\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[1;32m-> 3759\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3760\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3761\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\accelerate\\utils\\operations.py:820\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 820\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\accelerate\\utils\\operations.py:808\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 808\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\amp\\autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\accelerate\\utils\\operations.py:820\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 820\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\accelerate\\utils\\operations.py:808\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 808\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\amp\\autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\llama\\modeling_llama.py:863\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[0;32m    861\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 863\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m    866\u001b[0m     output \u001b[38;5;241m=\u001b[39m (logits,) \u001b[38;5;241m+\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m:]\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\loss\\loss_utils.py:36\u001b[0m, in \u001b[0;36mForCausalLMLoss\u001b[1;34m(logits, labels, vocab_size, num_items_in_batch, ignore_index, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mForCausalLMLoss\u001b[39m(\n\u001b[0;32m     33\u001b[0m     logits, labels, vocab_size: \u001b[38;5;28mint\u001b[39m, num_items_in_batch: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, ignore_index: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m     34\u001b[0m ):\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# Upcast to float if we need to compute the loss to avoid potential precision issues\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(logits\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# Shift so that tokens < n predict n\u001b[39;00m\n","\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.96 GiB. GPU "]}],"execution_count":null}]}