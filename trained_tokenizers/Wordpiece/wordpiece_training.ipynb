{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":263780,"sourceType":"datasetVersion","datasetId":110385}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin(token = \"YOUR_ID\")\nfrom transformers import AutoTokenizer\n\n\nllama_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\nprint(llama_tokenizer.all_special_tokens)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T19:19:44.681594Z","iopub.execute_input":"2025-03-29T19:19:44.681948Z","iopub.status.idle":"2025-03-29T19:19:45.398930Z","shell.execute_reply.started":"2025-03-29T19:19:44.681924Z","shell.execute_reply":"2025-03-29T19:19:45.398121Z"}},"outputs":[{"name":"stdout","text":"['<|begin_of_text|>', '<|end_of_text|>']\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"\nfrom tokenizers import Tokenizer, models, trainers, pre_tokenizers, processors\nfrom tokenizers.normalizers import Sequence, Lowercase, NFKC\nfrom tokenizers.pre_tokenizers import Whitespace\nfrom tokenizers.trainers import WordPieceTrainer\n\nfrom datasets import load_dataset\n\n\n\ndataset = load_dataset(\"Skylion007/openwebtext\", split=\"train\", streaming=True)\n\nnum_rows = 10_000_000 # Change the number if you want to take larger/smaller portion of data\ntexts = (example[\"text\"] for example in dataset.take(num_rows)) \n\n\ntokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\ntokenizer.normalizer = Sequence([NFKC(), Lowercase()])\ntokenizer.pre_tokenizer = Whitespace()\n\ntrainer = WordPieceTrainer(vocab_size=128256, special_tokens=['<|begin_of_text|>', '<|end_of_text|>'])\n# vocab size is equal to llama tokenizer vocab size\n\n\ntokenizer.train_from_iterator(texts, trainer)\n\ntokenizer_name = \"wordpiece_tokenizer\"\ntokenizer.save(f\"{tokenizer_name}.json\")\n\nprint(f\"Training is done. Tokenizer is saved as {tokenizer_name}.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T19:19:45.400072Z","iopub.execute_input":"2025-03-29T19:19:45.400361Z","iopub.status.idle":"2025-03-29T21:40:30.690032Z","shell.execute_reply.started":"2025-03-29T19:19:45.400338Z","shell.execute_reply":"2025-03-29T21:40:30.688992Z"}},"outputs":[{"name":"stderr","text":"'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 85f6d14a-06f2-4f40-93db-fddfeefef2b3)')' thrown while requesting GET https://huggingface.co/datasets/Skylion007/openwebtext/resolve/f3808c30e817981b845ec549c43e82bb467d8144/subsets/urlsf_subset18.tar\nRetrying in 1s [Retry 1/5].\n'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 335caf93-ac9f-4edc-a909-3bedddc3e711)')' thrown while requesting GET https://huggingface.co/datasets/Skylion007/openwebtext/resolve/f3808c30e817981b845ec549c43e82bb467d8144/subsets/urlsf_subset20.tar\nRetrying in 1s [Retry 1/5].\n","output_type":"stream"},{"name":"stdout","text":"Training is done. Tokenizer is saved as wordpiece_tokenizer.json\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"### Test the functionality","metadata":{}},{"cell_type":"code","source":"from tokenizers import Tokenizer\n\ntokenizer = Tokenizer.from_file(\"wordpiece_tokenizer.json\")\ntokenizer.get_vocab_size()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T21:40:30.691785Z","iopub.execute_input":"2025-03-29T21:40:30.692071Z","iopub.status.idle":"2025-03-29T21:40:30.923482Z","shell.execute_reply.started":"2025-03-29T21:40:30.692042Z","shell.execute_reply":"2025-03-29T21:40:30.922732Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"128256"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"test_sentences = [\n    \"The government is planning new policies.\",\n    \"AI models are becoming more powerful.\",\n    \"A quick brown fox jumps over the lazy dog.\"\n]\n\nfor sentence in test_sentences:\n    encoded = tokenizer.encode(sentence)\n    print(f\"Input: {sentence}\")\n\n    print(f\"Tokens: {encoded.tokens}\")\n    print(f\"IDs: {encoded.ids}\")\n    print(\"-\" * 50)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T22:03:25.595761Z","iopub.execute_input":"2025-03-29T22:03:25.596148Z","iopub.status.idle":"2025-03-29T22:03:25.603663Z","shell.execute_reply.started":"2025-03-29T22:03:25.596117Z","shell.execute_reply":"2025-03-29T22:03:25.602940Z"}},"outputs":[{"name":"stdout","text":"Input: The government is planning new policies.\nTokens: ['the', 'government', 'is', 'planning', 'new', 'policies', '.']\nIDs: [46345, 46914, 46385, 50176, 46539, 49811, 15]\n--------------------------------------------------\nInput: AI models are becoming more powerful.\nTokens: ['ai', 'models', 'are', 'becoming', 'more', 'powerful', '.']\nIDs: [53017, 50421, 46443, 50119, 46532, 49439, 15]\n--------------------------------------------------\nInput: A quick brown fox jumps over the lazy dog.\nTokens: ['a', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\nIDs: [40, 47922, 49015, 49540, 61897, 46633, 46345, 58745, 49709, 15]\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":18}]}